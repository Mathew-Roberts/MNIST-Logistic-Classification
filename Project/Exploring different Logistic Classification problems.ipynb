{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdd840b9ab44a389a644d4ff41790a9a",
     "grade": false,
     "grade_id": "cell-6ded9e10fb35754f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Logistic Regression Project\n",
    "\n",
    "In this mini project, I will be attempting to solve some logistic regression problems. This will start from a simple binary case with only two labels, progressing to a much larger classification problem using the MNIST handwritten images data set.\n",
    "Date: 18.11.2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bd7074d1695e70e8c0a936b29704da2",
     "grade": false,
     "grade_id": "cell-a633ccf06b277795",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b8af802f1a0231178e470649ea44639",
     "grade": false,
     "grade_id": "cell-1521ab82136c8e27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Binary logistic regression\n",
    "\n",
    "We first write the logistic regression function ***logistic_function*** that takes an argument named _inputs_ and returns the output of the logistic function, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{1 + \\exp(-x)} \\, ,\n",
    "\\end{align*}\n",
    "\n",
    "applied to the input. Here $x$ is the mathematical notation for the argument _inputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7a31ab463bfd733a5550c57433d06c3",
     "grade": false,
     "grade_id": "logistic-function",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def logistic_function(inputs):\n",
    "    value = 1/(1+np.exp(-(inputs)))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04ff095166cb1042179a534a57f7f5c4",
     "grade": false,
     "grade_id": "cell-8e2b052a692ae3ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Logistic regression is an optimisation problem, where we are required to minimise the associated error function. Below I have written two functions that implement the error function for binary logistic regression as well as to find its gradient. The function for the error function is named **binary_logistic_regression_cost_function** and should take the NumPy arrays _data_matrix_, _weights_ and _outputs_ as arguments. Here, _data_matrix_ is supposed to be a polynomial basis matrix (shown as $X$ in the equations below), while _weights_($w$) denotes the vector of weight parameters and _outputs_($Y$) is the vector of binary outputs (with values in $\\{0, 1\\}$). I have also written the **binary_logistic_regression_gradient** function, that takes the same inputs as **binary_logistic_regression_cost_function** and computes the gradient of the binary logistic regression cost function. This is required in order to minimise the cost function to obtain optimal parameters _weights_\n",
    "\n",
    "The loss function for binary logistic regression is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "L(w^k = \\frac{1}{2s} \\sum_{i=1}^{s}(f(w,x_{i}) - y_{i})^2),\n",
    "\\end{equation}\n",
    "\n",
    "and it's gradient is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla L(w^k) = X^T(\\sigma(Xw^k) - Y)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32f2b5001d7a1881ffa12451d5133cb8",
     "grade": false,
     "grade_id": "logistic-cost-function",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def polynomial_basis(inputs, degree=1):\n",
    "    \n",
    "    basis_matrix = np.ones((len(inputs), 1))\n",
    "    for counter in range(1, degree + 1):\n",
    "        basis_matrix = np.c_[basis_matrix, np.power(inputs, counter)]\n",
    "    return basis_matrix\n",
    "   \n",
    "\n",
    "def binary_logistic_regression_cost_function(data_matrix, weights, outputs):\n",
    "    s = data_matrix.shape[0]\n",
    "    values = []\n",
    "    for i in range(s):\n",
    "        result = np.log(1 + np.exp(data_matrix[i,:]@weights)) - (outputs[i]*(data_matrix[i,:]@weights))\n",
    "        values.append(result)\n",
    "    return np.sum(values)\n",
    "    \n",
    "    \n",
    "def binary_logistic_regression_gradient(data_matrix, weights, outputs):\n",
    "    result = data_matrix.T @ (logistic_function(data_matrix@weights) - outputs)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "766d8951af9708ef7c8c6be80d721575",
     "grade": false,
     "grade_id": "cell-ee5c7e3b09c4f3f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "The above minimisation problem is non-linear, therefore we cannot simply set the gradient = 0 and solve for $w$. Instead, we must use a gradient descent algorithm to incrementally reach the minimum of our error function. The gradient descent algorithm works iteratively, by continually updating and reusing the _weights_ array to minimise the error function. A mathematical representation is given by,\n",
    "\n",
    "\\begin{equation}\n",
    "w^{k+1} = w^k - \\tau\\nabla L(w^k)\n",
    "\\end{equation}\n",
    "where each time the new weights $w^k$ are created, we progress closer to the $w$ which minimises our error function.\n",
    "\n",
    "The gradient descent arguments _objective_ and _gradient_ are lambda functions that can take _weight_ arrays as arguments and return the scalar value of the error function, and the array representation of the corresponding gradient respectively. The argument _initial_weights_ specifies the initial value of weights (so-called $w^{k=0}$) over which you iterate. The argument _step_size_ is the gradient descent step-size parameter, the argument _no_of_iterations_ specifies the maximum number of iterations, and _print_output_ determines after how many iterations the function produces a text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a0df2f9342e9204a48960b5885f38d6",
     "grade": false,
     "grade_id": "gradient_descent",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(objective, gradient, initial_weights, step_size=1, no_of_iterations=100, print_output=10):\n",
    "    objective_values = []\n",
    "    weights = np.copy(initial_weights)\n",
    "    objective_values.append(objective(weights))\n",
    "    \n",
    "    for counter in range(no_of_iterations):\n",
    "        weights -= step_size * gradient(weights)\n",
    "        objective_values.append(objective(weights))\n",
    "        \n",
    "        if (counter + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=counter+1, \\\n",
    "                    m=no_of_iterations, o=objective_values[counter]))\n",
    "            \n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=counter + 1, \\\n",
    "                m=no_of_iterations, o=objective_values[counter]))\n",
    "    \n",
    "    return weights, objective_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90e8d7a7ec95458b09266c3d92aaaec6",
     "grade": false,
     "grade_id": "cell-c3a6dd3a899dcb5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following cell I have written a function **standardise** that standardises the columns of a two-dimensional NumPy array _data_matrix_. This results in a better classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "924547dca886c6e5532970ba42bdb82c",
     "grade": false,
     "grade_id": "standardise",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def standardise(data_matrix):\n",
    "    row_of_means = np.mean(data_matrix,axis = 0)\n",
    "    standardised_matrix = data_matrix - row_of_means\n",
    "    row_of_stds = np.std(standardised_matrix, axis = 0)\n",
    "  \n",
    "    return (standardised_matrix/row_of_stds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "faa82c0c746059f5e089fd2a8bc80f46",
     "grade": false,
     "grade_id": "cell-4ec03cc4fda49034",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Height-Weight Classifier\n",
    "\n",
    "I will first train a simple binary classifier. The following cell initialises some data containing the information about the height and weight of a sample population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40ef881fba829b6964f8b1d65fcb6a47",
     "grade": false,
     "grade_id": "cell-d3175e8ccd63fab4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_loader import load_data\n",
    "height, weight, biological_sex = load_data()\n",
    "biological_sex = np.double(biological_sex).reshape(-1, 1)\n",
    "plt.plot(height, weight, '.', linewidth=3)\n",
    "plt.xlabel('Height in metres', fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.ylabel('Weight in kilogram', fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tight_layout;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93ddcbc533969ca2585a24d87b6f10ff",
     "grade": false,
     "grade_id": "cell-1d8cb3da4dac45fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following cell, I have initialised a polynomial data matrix _data_matrix_ of degree one with the standardised inputs formed from the height and weight arrays of the dataset. The lambda function _objective_ function with argument _weights_ is based on the **binary_logistic_regression_cost_function** with fixed arguments _data_matrix_ and _biological_sex_. The gradient function was created similarly using the **binary_logistic_regression_gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7d63d2842004f750b77a6454bf5006d",
     "grade": false,
     "grade_id": "cell-65673f36b848eab4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#creating the polynomial basis array\n",
    "height_standardised = standardise(height)\n",
    "weight_standardised = standardise(weight)\n",
    "weight_height_std = np.c_[height_standardised,weight_standardised]\n",
    "data_matrix = polynomial_basis(weight_height_std)\n",
    "\n",
    "#creating the cost function and gradient\n",
    "objective = lambda weights: binary_logistic_regression_cost_function(data_matrix, weights, biological_sex)\n",
    "gradient = lambda weights: binary_logistic_regression_gradient(data_matrix, weights, biological_sex)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "726095861ab7131322e489c8a33d15f5",
     "grade": false,
     "grade_id": "cell-5c25ca5d2e19856b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now I will call gradient descent with the following cell to compute _optimal_weights_ for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7753f6ed0ac1f5a4adbb1ef268caae81",
     "grade": false,
     "grade_id": "cell-fb31b6af5bbbcc9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000/7000, objective = 2091.9568591203943.\n",
      "Iteration 2000/7000, objective = 2091.30072155257.\n",
      "Iteration 3000/7000, objective = 2091.297983425523.\n",
      "Iteration 4000/7000, objective = 2091.2979712556394.\n",
      "Iteration 5000/7000, objective = 2091.2979712013266.\n",
      "Iteration 6000/7000, objective = 2091.297971201084.\n",
      "Iteration 7000/7000, objective = 2091.297971201083.\n",
      "Iteration completed after 7000/7000, objective = 2091.297971201083.\n"
     ]
    }
   ],
   "source": [
    "initial_weights = np.zeros((data_matrix.shape[1], 1))\n",
    "optimal_weights, objective_values = gradient_descent(objective, gradient, initial_weights, \\\n",
    "                                    step_size=1.9/(np.linalg.norm(data_matrix, 2) ** 2), \\\n",
    "                                    no_of_iterations=7000, print_output=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0102eed55ffb0f81e5ba3f42cffd9422",
     "grade": true,
     "grade_id": "cell-8090fbc611c0a194",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal weights are w = [[-0.01870311  1.89527459 -6.3680829 ]].T with objective value L(w) = 2091.297971201083.\n"
     ]
    }
   ],
   "source": [
    "print(\"The optimal weights are w = {w}.T with objective value L(w) = {o}.\".format(w = optimal_weights.T, \\\n",
    "        o=objective_values[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14f46ce484ad38b6e849796544e9ad6c",
     "grade": false,
     "grade_id": "cell-18f1b30c908aaae8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below, I have written two functions **prediction_function** and **classification_accuracy** that turn my predicitons into classification results and that compare how many labels have been classified correctly. The function **prediction_function** takes the arguments _data_matrix_ and _weights_ as inputs and returns a vector of class labels with binary values in $\\{0, 1\\}$ as its output. The function **classification_accuracy** takes two inputs _true_labels_ and _recovered_labels_ and returns the percentage of correctly classified labels divided by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dda3261c6408de6bfac72ee8a2e3d197",
     "grade": false,
     "grade_id": "prediction-and-accuracy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def prediction_function(data_matrix, weights):\n",
    "    results = logistic_function(data_matrix @ weights)\n",
    "    binary = np.where(results >0.5,1,0)\n",
    "    return binary\n",
    "   \n",
    "def classification_accuracy(true_labels, recovered_labels):\n",
    "    equal_labels = (recovered_labels == true_labels)\n",
    "    return np.mean(equal_labels)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "977c57bb1188016adfa151474f8baecf",
     "grade": true,
     "grade_id": "accuracy-test",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the training set is 91.94 %.\n"
     ]
    }
   ],
   "source": [
    "print(\"The classification accuracy for the training set is {p} %.\".format(p = 100 * \\\n",
    "        classification_accuracy(biological_sex, prediction_function(data_matrix, optimal_weights))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00633505cdd1f8712777a761fbe95961",
     "grade": false,
     "grade_id": "cell-66ec567aec881880",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Multinomial logistic regression\n",
    "\n",
    "This concludes the binary classification part of the project. We now move on to multinomial logistic regression for multi-class classfication problems.\n",
    "\n",
    "I have initially implemented a softmax function **softmax_function**; this acts similarly to the logistic function in the binary case, however this function allows us to tackle classification problems with more than 2 labels. The function takes the NumPy array _argument_ as its main argument, but also has an optional _axis_ argument to determine across which array-dimension you apply the softmax operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4977cb40c6c166eebb31fe6ef824918a",
     "grade": false,
     "grade_id": "softmax",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_function(argument, axis=None):\n",
    "    \n",
    "    if argument.ndim == 1:\n",
    "        new_array = np.exp(argument)\n",
    "        result = new_array/np.sum(new_array)\n",
    "        \n",
    "        return result\n",
    "    elif argument.ndim > 1:\n",
    "        \n",
    "        if axis == None:\n",
    "            result = np.exp(argument)/sum(sum(np.exp(x)) for x in argument)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        elif axis == 0:\n",
    "            new_array = np.exp(argument)\n",
    "            result = new_array/new_array.sum(axis=0, keepdims = True)\n",
    "            \n",
    "            return result\n",
    "                \n",
    "        elif axis == 1:\n",
    "            new_array = np.exp(argument)\n",
    "            result = new_array/new_array.sum(axis=1, keepdims = True)\n",
    "          \n",
    "            return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22c6fcff95effedaa57877d34de89392",
     "grade": false,
     "grade_id": "cell-941464e24613e0a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, I have written a function **one_hot_vector_encoding** that converts an NumPy array _labels_ with values in the range of $\\{0, K - 1\\}$ into so-called one-hot vector encodings. For example, for $K = 3$ and a label vector $\\text{labels} = \\left( \\begin{matrix} 2 & 0 & 1 & 2\\end{matrix} \\right)^\\top$, the output of **one_hot_vector_encoding(labels)** should be a two-dimensional NumPy array of the form\n",
    "\n",
    "\\begin{align*}\n",
    "\\left( \\begin{matrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{matrix} \\right) \\, . \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94bcefd8f6f2834778c5e510f9797c83",
     "grade": false,
     "grade_id": "one-hot-vector-encoding",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_vector_encoding(labels):\n",
    "    labels_copy = labels\n",
    "    ohv_matrix = np.zeros((labels_copy.size, labels_copy.max()+1))\n",
    "    ohv_matrix[np.arange(labels_copy.size),labels_copy] = 1\n",
    "    return ohv_matrix   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "740154e524df161eed6ffcb3b6daf01f",
     "grade": false,
     "grade_id": "cell-4d72a85842f6c692",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below I implement the error function and gradient for the multinomial logistic regression in terms of two functions **multinomial_logistic_regression_cost_function** and **multinomial_logistic_regression_gradient**. The mathematical representation of the cost function is as follows,\n",
    "\\begin{equation}\n",
    "L(W^k) := \\sum_{i=1}^s \\mathrm{log}(\\sum_{i=1}^K \\mathrm{exp}(\\langle\\phi(x_i),w_j\\rangle)) - \\sum_{i=1}^s\\sum_{j=1}^K1_{y_i=j}\\langle\\phi(x_i),w_j\\rangle\n",
    "\\end{equation}\n",
    "\n",
    "and the gradient of the above is given by,\n",
    "\\begin{equation}\n",
    "\\nabla L(w^k) = X^T(\\sigma(Xw^k) - Y_{ohv})\n",
    "\\end{equation}\n",
    "where $\\sigma$ is the softmax function, and $Y_{ohv}$ is the one hot vector representation of the the labels. \n",
    "\n",
    "\n",
    "As in the binary classification case, the arguments are the polynomial data matrix _data_matrix_ and weights that are now named _weight_matrix_. Instead of passing on labels as _outputs_ as in the binary case, I will pass the one hot vector encoding representation _one_hot_vector_encodings_ as my third argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6866c7079139fd8a29d4c148a6e2f034",
     "grade": false,
     "grade_id": "multinomial_logistic_regression",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multinomial_logistic_regression_cost_function(data_matrix, weight_matrix, one_hot_vector_encodings):\n",
    "    #loss function\n",
    "    s = data_matrix.shape[0]\n",
    "    K = one_hot_vector_encodings.shape[1]\n",
    "    values = 0\n",
    "    \n",
    "    for i in range(s):\n",
    "        term1s = []\n",
    "        col = np.where(one_hot_vector_encodings[i,:] == 1)[0][0]\n",
    "        term2 = (np.dot(data_matrix[i,:],weight_matrix[:,col]))\n",
    "        for j in range(K): \n",
    "            term1_before = (np.exp(np.dot(data_matrix[i,:],weight_matrix[:,j])))                  \n",
    "            term1s.append(term1_before)\n",
    "        term1 = np.log((np.sum(term1s)))\n",
    "        values += term1 - term2\n",
    "    return values\n",
    "    \n",
    "def multinomial_logistic_regression_gradient(data_matrix, weight_matrix, one_hot_vector_encodings):\n",
    "    result = data_matrix.T @ (softmax_function(data_matrix@weight_matrix,axis = 1) - one_hot_vector_encodings)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a034361a28a9e882f7d7ad4328070a9c",
     "grade": false,
     "grade_id": "cell-b810132e15131dc9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "I will initially test my functions on the [UCI wine dataset](https://archive.ics.uci.edu/ml/datasets/wine); the dataset contains 13 attributes from a chemical analysis of Italian wines from three different cultivars. The code in the following cell loads the dataset and stores the labels in a NumPy array _labels_ and the attributes in a NumPy array _inputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c66a8b68f9903af0351c13f30605934f",
     "grade": false,
     "grade_id": "cell-d080a23a2048227b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "wines = np.loadtxt('wine.data', delimiter=',')\n",
    "labels = wines[:, 0].astype(int) - 1\n",
    "inputs = wines[:, 1::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a9db194ed9b4c4b458c753d58b0196e",
     "grade": false,
     "grade_id": "cell-d26a44b054d22e4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below, I will transform the labels _labels_ into a one hot vector representation with my function **one_hot_vector_encoding** and store my results in a NumPy array named _outputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9378cf561d0799b518319252eaeb6c87",
     "grade": false,
     "grade_id": "cell-7362e79b5354d63b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "outputs = one_hot_vector_encoding(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a365aa01296ecd58f3550a5a83dc403d",
     "grade": false,
     "grade_id": "cell-100a0ea7469b3821",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following cell, I have written code that initialises a polynomial data matrix _data_matrix_ of degree one with the standardised inputs _inputs_ from the wine dataset. I define an objective function _objective_ with argument _weight_matrix_ based on the **multinomial_logistic_regression_cost_function** with fixed arguments _data_matrix_ and _one_hot_vector_encodings_. In a similar fashion, I create a function _gradient_ based on **multinomial_logistic_regression_gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "712836b79b8e6389ce84a45f8439a300",
     "grade": false,
     "grade_id": "cell-094a5bddd94a009a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "data_matrix = polynomial_basis(standardise(inputs))\n",
    "\n",
    "objective = lambda weight_matrix: multinomial_logistic_regression_cost_function(data_matrix, weight_matrix, outputs) \n",
    "gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(data_matrix, weight_matrix, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d06a10bf709742803be0723fd183611d",
     "grade": false,
     "grade_id": "cell-901f918df8fdef00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Call gradient descent with the following cell to compute an _optimal_weight_matrix_ for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acf5d4074e6b0b65252ef485d45d5838",
     "grade": false,
     "grade_id": "cell-175babea73e2db36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000/100000, objective = 0.37363264592740286.\n",
      "Iteration 10000/100000, objective = 0.19859836381135887.\n",
      "Iteration 15000/100000, objective = 0.13605627122241737.\n",
      "Iteration 20000/100000, objective = 0.10372479744703211.\n",
      "Iteration 25000/100000, objective = 0.08391801536513466.\n",
      "Iteration 30000/100000, objective = 0.07051872085948618.\n",
      "Iteration 35000/100000, objective = 0.06084138658523397.\n",
      "Iteration 40000/100000, objective = 0.05351961411115225.\n",
      "Iteration 45000/100000, objective = 0.047784005082136094.\n",
      "Iteration 50000/100000, objective = 0.04316789618475969.\n",
      "Iteration 55000/100000, objective = 0.03937164510108371.\n",
      "Iteration 60000/100000, objective = 0.036193967173282715.\n",
      "Iteration 65000/100000, objective = 0.0334945959405184.\n",
      "Iteration 70000/100000, objective = 0.031172765703073146.\n",
      "Iteration 75000/100000, objective = 0.02915420629986798.\n",
      "Iteration 80000/100000, objective = 0.027382962414542966.\n",
      "Iteration 85000/100000, objective = 0.02581606984776652.\n",
      "Iteration 90000/100000, objective = 0.024419988124662062.\n",
      "Iteration 95000/100000, objective = 0.02316814830816938.\n",
      "Iteration 100000/100000, objective = 0.022039229212158062.\n",
      "Iteration completed after 100000/100000, objective = 0.022039229212158062.\n"
     ]
    }
   ],
   "source": [
    "initial_weight_matrix = np.zeros((data_matrix.shape[1], outputs.shape[1]))\n",
    "optimal_weight_matrix, objective_values = gradient_descent(objective, gradient, initial_weight_matrix, \\\n",
    "                                    step_size=1.9/(np.linalg.norm(data_matrix, 2) ** 2), \\\n",
    "                                    no_of_iterations=100000, print_output=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5a8d433e17eb215eab8b141e3fa48bd",
     "grade": false,
     "grade_id": "cell-b7bf0daf68390bac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The below function **multinomial_prediction_function** turns my predicitons into labels. The function takes the arguments _data_matrix_ and _weight_matrix_ as inputs and returns a vector of labels with values in $\\{0, K - 1 \\}$ as its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e824634474ea1ee537d80abc1bd092cd",
     "grade": false,
     "grade_id": "cell-2da6bb5a7e17ea5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multinomial_prediction_function(data_matrix, weight_matrix):\n",
    "    k = weight_matrix.shape[1]\n",
    "    matrix = logistic_function(data_matrix@weight_matrix)\n",
    "    ohv_matrix = (matrix == matrix.max(axis=1,keepdims = 1)).astype(float)\n",
    "    prediction = ohv_matrix @ np.array(np.arange(0,k))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbf3d6b935f497fab4bd93eeea119319",
     "grade": true,
     "grade_id": "cell-49fe4e66273baf80",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the wine training set is 100.0 %.\n"
     ]
    }
   ],
   "source": [
    "print(\"The classification accuracy for the wine training set is {p} %.\".format(p = 100 * \\\n",
    "        classification_accuracy(labels, multinomial_prediction_function(data_matrix, optimal_weight_matrix))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba0705811c857f5094055fbd9f385917",
     "grade": false,
     "grade_id": "cell-45dcd721e2869a9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ridge logistic regression\n",
    "\n",
    "For the next parts we will consider using variational regularisation to obtain a better generalised model. I have modified the multinomial logistic regression problem to include a squared Frobenius norm of the weights as a regularisation term. I have then written two functions **ridge_logistic_regression_cost_function** and **ridge_logistic_regression_gradient** that take the arguments _data_matrix_, _weight_matrix_, _one_hot_vector_encodings_ and _regularisation_parameter_ as inputs. The function **ridge_logistic_regression_cost_function** returns the evulation of the multinomial logistic regression cost function with its linear model being determined by the polynomial basis matrix _data_matrix_ and the weight matrix _weight_matrix_, plus _regularisation_parameter_ times the squared Frobenius norm of _weight_matrix_ divided by two. The function **ridge_logistic_regression_gradient** computes the corresponding gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acdbb33c938f33ee566b9dee3262d846",
     "grade": false,
     "grade_id": "cell-abadca310df6699f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ridge_logistic_regression_cost_function(data_matrix, weight_matrix, one_hot_vector_encodings, \\\n",
    "                                            regularisation_parameter):\n",
    "    ridge_term = (regularisation_parameter/2) * (np.linalg.norm(weight_matrix)**2)\n",
    "    return multinomial_logistic_regression_cost_function(data_matrix, weight_matrix, one_hot_vector_encodings) + ridge_term\n",
    "    \n",
    "    \n",
    "    \n",
    "def ridge_logistic_regression_gradient(data_matrix, weight_matrix, one_hot_vector_encodings, \\\n",
    "                                       regularisation_parameter):\n",
    "    ridge_term = regularisation_parameter * weight_matrix\n",
    "    return multinomial_logistic_regression_gradient(data_matrix,weight_matrix,one_hot_vector_encodings) + ridge_term \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0996e4e5192b31e3a4934a13a883b09",
     "grade": false,
     "grade_id": "cell-1af3c4a5f531e2fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "I have set my regularisation parameter _regularisation_parameter_ to the value 15 and defined an objective function _objective_ as well as a gradient function _gradient_, both with argument _weight_matrix_, for fixed _data_matrix_ and _outputs_ as from the wine dataset that I used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f40622d6042d8793ce03206d450ee648",
     "grade": false,
     "grade_id": "cell-3f1796626df02a88",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "regularisation_parameter = 15\n",
    "objective = lambda weight_matrix: ridge_logistic_regression_cost_function(data_matrix, weight_matrix, outputs,regularisation_parameter) \n",
    "gradient = lambda weight_matrix: ridge_logistic_regression_gradient(data_matrix, weight_matrix, outputs,regularisation_parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d5eec1441b9057a16855a9953bfe215",
     "grade": false,
     "grade_id": "cell-4ac1dd08e2ba7c80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/100, objective = 50.66582189248284.\n",
      "Iteration 20/100, objective = 48.11155251656332.\n",
      "Iteration 30/100, objective = 47.821933497251294.\n",
      "Iteration 40/100, objective = 47.766837844563554.\n",
      "Iteration 50/100, objective = 47.7524214796791.\n",
      "Iteration 60/100, objective = 47.747879295725696.\n",
      "Iteration 70/100, objective = 47.7462923106324.\n",
      "Iteration 80/100, objective = 47.74570110157326.\n",
      "Iteration 90/100, objective = 47.745470493345195.\n",
      "Iteration 100/100, objective = 47.745377257110924.\n",
      "Iteration completed after 100/100, objective = 47.745377257110924.\n"
     ]
    }
   ],
   "source": [
    "initial_weight_matrix = np.zeros((data_matrix.shape[1], outputs.shape[1]))\n",
    "ridge_weight_matrix, ridge_objective_values = gradient_descent(objective, gradient, initial_weight_matrix, \\\n",
    "                                    step_size=1.9/np.linalg.norm(data_matrix.T @ data_matrix + \\\n",
    "                                    regularisation_parameter * np.eye(data_matrix.shape[1]), 2), \\\n",
    "                                    no_of_iterations=100, print_output=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a093e26a90163695684c1f98e8cfd645",
     "grade": true,
     "grade_id": "cell-3d84066ed6f97945",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ridge regression classification accuracy with regularisation parameter 15 for the wine training set is 99.43820224719101 %.\n"
     ]
    }
   ],
   "source": [
    "print(\"The ridge regression classification accuracy with regularisation parameter {a}\".format(a = \\\n",
    "       regularisation_parameter), \"for the wine training set is {p} %.\".format(p = 100 * \\\n",
    "        classification_accuracy(labels, multinomial_prediction_function(data_matrix, ridge_weight_matrix))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e59581e2eed10dab4a2d0ca63e5e67d",
     "grade": false,
     "grade_id": "cell-681ea7611e1b8d47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## LASSO-type logistic regression\n",
    "\n",
    "As a final exercise, I will consider LASSO-type regularisation. Here, I implement a modification of the multinomial logistic regression problem that contains a positive multiple of the one-norm of the weight matrix as regularisation term, and approximate a solution numerically with the proximal gradient descent method. The proximal method must be used due to the one norm being non-differentiable.\n",
    "\n",
    "I begin by completing the following three functions. The function **soft_thresholding** takes the two arguments _argument_ and _threshold_ and returns the solution of the soft-thresholding operation applied to _argument_ with threshold _threshold_. \n",
    "\n",
    "The function **lasso_logistic_regression_cost_function** implements the multinomial logistic regression loss with additional one-norm regularisation for polynomial basis matrix _data_matrix_, weight matrix _weight_matrix_, the one hot vector encoding output _one_hot_vector_encodings_ and the regularisation parameter _regularisation_parameter_.\n",
    "\n",
    "The function **proximal_gradient_descent** takes the same arguments as the function **gradient_descent**, with additional argument _proximal_map_ in order to specify the proximal map to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdab43cdab7db63c950b7d0a6c06e564",
     "grade": false,
     "grade_id": "cell-b2608195afc72125",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def soft_thresholding(argument, threshold):\n",
    "    return np.sign(argument) * np.maximum(0, np.abs(argument) - threshold)\n",
    "    \n",
    "def lasso_logistic_regression_cost_function(data_matrix, weight_matrix, one_hot_vector_encodings, \\\n",
    "                                       regularisation_parameter):\n",
    "    lasso_term = regularisation_parameter * np.linalg.norm(weight_matrix,1)\n",
    "    return multinomial_logistic_regression_cost_function(data_matrix, weight_matrix, one_hot_vector_encodings) + lasso_term\n",
    "    \n",
    "def proximal_gradient_descent(objective, gradient, proximal_map, initial_weights, step_size=1, \\\n",
    "                              no_of_iterations=100, print_output=100):\n",
    "    objective_values = []\n",
    "    weights = initial_weights    \n",
    "    objective_values.append(objective(weights))\n",
    "    for counter in range(no_of_iterations):\n",
    "        weights = proximal_map(weights - step_size * gradient(weights))\n",
    "        objective_values.append(objective(weights))\n",
    "        if (counter + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=counter+1, \\\n",
    "                    m=no_of_iterations, o=objective_values[counter]))\n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=counter + 1, \\\n",
    "                m=no_of_iterations, o=objective_values[counter]))\n",
    "    return weights, objective_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bfbe1872c304938510aeafc7fa10c25",
     "grade": false,
     "grade_id": "cell-e3ac148e879c476f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the next cell, I define a suitable objective function _objective_ and a suitable gradient function _gradient_, both with argument _weight_matrix_ and fixed _data_matrix_ and _outputs_ from the wine dataset we have seen before. I have also defined a suitable proximal map function _proximal_map_ with correctly chosen threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5af740fe918364e9026fad9423073f3d",
     "grade": false,
     "grade_id": "cell-3afeb025acbc0916",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "regularisation_parameter = 0.5\n",
    "step_size = 1.9/(np.linalg.norm(data_matrix, 2) ** 2)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "objective = lambda weight_matrix: lasso_logistic_regression_cost_function(data_matrix, weight_matrix, outputs, \\\n",
    "                                       regularisation_parameter)\n",
    "gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(data_matrix, weight_matrix, outputs)\n",
    "\n",
    "proximal_map = lambda weights: soft_thresholding(weights, regularisation_parameter * \\\n",
    "                                                 step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "998771495b551e63b22c8b1980b179c9",
     "grade": false,
     "grade_id": "cell-33117d62e5699048",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000/10000, objective = 8.24959786201634.\n",
      "Iteration 2000/10000, objective = 8.263886291899661.\n",
      "Iteration 3000/10000, objective = 8.320757757764321.\n",
      "Iteration 4000/10000, objective = 8.354235362086085.\n",
      "Iteration 5000/10000, objective = 8.374118326110752.\n",
      "Iteration 6000/10000, objective = 8.385347232389174.\n",
      "Iteration 7000/10000, objective = 8.392015832216455.\n",
      "Iteration 8000/10000, objective = 8.396214878066097.\n",
      "Iteration 9000/10000, objective = 8.399020949261885.\n",
      "Iteration 10000/10000, objective = 8.40100378273745.\n",
      "Iteration completed after 10000/10000, objective = 8.40100378273745.\n"
     ]
    }
   ],
   "source": [
    "initial_weight_matrix = np.zeros((data_matrix.shape[1], outputs.shape[1]))\n",
    "lasso_weight_matrix, lasso_objective_values = proximal_gradient_descent(objective, gradient, proximal_map, \\\n",
    "                                                initial_weight_matrix, step_size, no_of_iterations=10000, \\\n",
    "                                                print_output=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0f049b162367cd9d765fb16be5a693e",
     "grade": true,
     "grade_id": "cell-4cd3a07610421486",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lasso classification accuracy with regularisation parameter 0.5 for the wine training set is 100.0 %.\n"
     ]
    }
   ],
   "source": [
    "print(\"The lasso classification accuracy with regularisation parameter {a}\".format(a = \\\n",
    "       regularisation_parameter), \"for the wine training set is {p} %.\".format(p = 100 * \\\n",
    "        classification_accuracy(labels, multinomial_prediction_function(data_matrix, lasso_weight_matrix))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6ffccf3240508413da555b379ba46e8",
     "grade": false,
     "grade_id": "cell-cf4145537166f52b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## MNIST free-style\n",
    "\n",
    "We begin by loading the MNIST training set that is taken from this [source](http://yann.lecun.com/exdb/mnist/):\n",
    "This consists of 60000 training images of handwritten digits and their corresponding labels.\n",
    "An example data point from the MNIST set has been visualised below - I will be using the above logistic regression approaches to create a sufficiently high performing classifying algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f85db745293111420d8bafb5f6dcc87",
     "grade": false,
     "grade_id": "cell-7b42fe502f206f1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MNIST training set contains 60000 images with 784 pixels each.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_loader import load_mnist\n",
    "images, labels = load_mnist('')\n",
    "print(\"The MNIST training set contains {s} images with {p} pixels each.\".format(s = images.shape[0], \\\n",
    "        p = images.shape[1]))\n",
    "plt.imshow(images[13,:].reshape(28, 28))\n",
    "plt.title(\"This is the 13th image of the MNIST training set. The corresponding label is {l}\".format( \\\n",
    "            l=labels[13]))\n",
    "plt.tight_layout;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is a copy of the gradient descent formula, however I have removed the objective values from the return command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_mnist(objective, gradient, initial_weights, step_size=1, no_of_iterations=100, print_output=100):\n",
    "    objective_values = []\n",
    "    weights = np.copy(initial_weights)\n",
    "    objective_values.append(objective(weights))\n",
    "    \n",
    "    for counter in range(no_of_iterations):\n",
    "        weights -= step_size * gradient(weights)\n",
    "        objective_values.append(objective(weights))\n",
    "        \n",
    "        if (counter + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=counter+1, \\\n",
    "                    m=no_of_iterations, o=objective_values[counter]))\n",
    "            \n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=counter + 1, \\\n",
    "                m=no_of_iterations, o=objective_values[counter]))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### REMEMBER\n",
    "s = 60000\n",
    "j = 784 + 1\n",
    "k = 10\n",
    "\n",
    "data_matrix = s x j\n",
    "\n",
    "weight_matrix =  j x k\n",
    "\n",
    "labels = s x k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cells will standardise the images matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_mnist(data_matrix):\n",
    "    datamatrix_copy = np.copy(data_matrix)\n",
    "    \n",
    "    row_of_means = np.mean(datamatrix_copy,axis = 0)\n",
    "    standardised_matrix = datamatrix_copy - row_of_means\n",
    "   \n",
    "    Array = np.copy(standardised_matrix)\n",
    "    \n",
    "    for col in range(datamatrix_copy.shape[1]):\n",
    "        if np.std(standardised_matrix[:,col]) == 0:\n",
    "            Array[:,col] = standardised_matrix[:,col]/1\n",
    "        else:\n",
    "            Array[:,col] = standardised_matrix[:,col]/\\\n",
    "                                                    np.std(standardised_matrix[:,col])\n",
    "    \n",
    "  \n",
    "    return Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------- CREATING NEW VARIABLES FROM THE DATA -----------------------------------------\n",
    "\n",
    "mnist_inputs = standardise_mnist(images)\n",
    "mnist_outputs = labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Partitioning (Training/Test/Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a partition of the data. 20% will be used for validation, whilst the other 80% will be used for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Partition(dataset,labels,split = 0.20):\n",
    "    dataset_copy = list(dataset)\n",
    "    labels_copy = list(labels)\n",
    "    val_data = []\n",
    "    val_labels = []\n",
    "    validation_set_size = split * len(dataset_copy)\n",
    "    while len(val_data) < validation_set_size:\n",
    "        index = np.random.randint(len(dataset_copy))\n",
    "        val_data.append(dataset_copy.pop(index))\n",
    "        val_labels.append(labels_copy.pop(index))\n",
    "    train_test_data = dataset_copy\n",
    "    train_test_labels = labels_copy\n",
    "    \n",
    "    return np.array(val_data), np.array(val_labels), np.array(train_test_data), np.array(train_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the splitting of the validation and training/testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, val_labels, train_test_data, train_test_labels = Data_Partition(mnist_inputs, mnist_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the data for the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data,val_labels\n",
    "\n",
    "#--------------------------------- CREATING POLYNOMIAL BASIS  -------------\n",
    "\n",
    "val_data_matrix = polynomial_basis(val_data,degree = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are further spliting up the test_train set, into separate testing and training arrays. These will be done randomly, but all models will be trained on the same test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data, train_test_labels # These are training/testing set before partition (48,000 samples)\n",
    "\n",
    "#--------------------------------- CREATING POLYNOMIAL BASIS AND ONE HOT ENCODINGS MATRICES -------------\n",
    "\n",
    "train_test_data_matrix = polynomial_basis(train_test_data, degree = 1)\n",
    "train_test_ohv = one_hot_vector_encoding(train_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_partition(inputdata,outputdata, prop = 0.7):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    no_training_data = prop * len(inputdata)\n",
    "    inputs = list(inputdata)\n",
    "    outputs = list(outputdata)\n",
    "    while len(train_data) < no_training_data:\n",
    "        indx = np.random.randint(len(inputs))\n",
    "        train_labels.append(outputs.pop(indx))\n",
    "        train_data.append(inputs.pop(indx))\n",
    "    test_data = inputs\n",
    "    test_labels = outputs\n",
    "    return np.array(train_data), np.array(train_labels), np.array(test_data), np.array(test_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the splits for the training/testing partition created by the data_partition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = train_test_partition(train_test_data_matrix,train_test_ohv) #Split sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model A: Ridge Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block only uses the training and testing data sets as defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation_ridge(regularisation_parameters,traindata = train_data, trainlabels = train_labels,\\\n",
    "                           testdata = test_data,testlabels = test_labels, iterations = 1000):\n",
    "\n",
    "    \n",
    "#----------------------------- TEST RESULTS FOR EACH PARAMETER ---------------------------------    \n",
    "    \n",
    "    \n",
    "    parameters = []\n",
    "    classification_accuracies = []\n",
    "    optimal_weight_matrices = []\n",
    "      \n",
    "    \n",
    "#---------------------------- TRAINING EACH MODEL WITH THE TRAINING SET -------------------------------    \n",
    "    \n",
    "    \n",
    "    for alpha in regularisation_parameters:\n",
    "        parameters.append(alpha)\n",
    "        \n",
    "        initial_weight_matrix = np.zeros((traindata.shape[1], trainlabels.shape[1]))\n",
    "\n",
    "        objective = lambda weight_matrix: ridge_logistic_regression_cost_function(traindata,\\\n",
    "                                                                        weight_matrix, trainlabels,alpha) \n",
    "        gradient = lambda weight_matrix: ridge_logistic_regression_gradient(traindata, \\\n",
    "                                                                            weight_matrix, trainlabels,alpha)\n",
    "        \n",
    "        optimal_weight_matrix = gradient_descent_mnist(objective, gradient, initial_weight_matrix, \\\n",
    "                                          step_size=1.9/np.linalg.norm(traindata.T @ traindata + \\\n",
    "                                            alpha * np.eye(traindata.shape[1]), 2), \\\n",
    "                                                no_of_iterations= iterations, print_output=10)\n",
    "        \n",
    "        optimal_weight_matrices.append(optimal_weight_matrix)\n",
    "        \n",
    "        \n",
    "#---------------------------- TESTING THE MODEL ON TEST LABELS -------------------------------\n",
    "        \n",
    "    \n",
    "        classifier = classification_accuracy(testlabels@np.arange(0,testlabels.shape[1]), \\\n",
    "                            multinomial_prediction_function(testdata, optimal_weight_matrix))\n",
    "            \n",
    "        classification_accuracies.append(classifier)\n",
    "        \n",
    "    return list(zip(parameters,classification_accuracies)), optimal_weight_matrices[np.argmax(classification_accuracies)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing 3 alpha parameters, 1,10 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/500, objective = 31373.83287106924.\n",
      "Iteration 20/500, objective = 22885.43367523952.\n",
      "Iteration 30/500, objective = 19442.286545863844.\n",
      "Iteration 40/500, objective = 17517.56167407681.\n",
      "Iteration 50/500, objective = 16264.004278608469.\n",
      "Iteration 60/500, objective = 15370.146946898285.\n",
      "Iteration 70/500, objective = 14693.45680186876.\n",
      "Iteration 80/500, objective = 14158.990240055671.\n",
      "Iteration 90/500, objective = 13723.334293645576.\n",
      "Iteration 100/500, objective = 13359.475727956336.\n",
      "Iteration 110/500, objective = 13049.654369797829.\n",
      "Iteration 120/500, objective = 12781.672024085592.\n",
      "Iteration 130/500, objective = 12546.847716262611.\n",
      "Iteration 140/500, objective = 12338.819016643203.\n",
      "Iteration 150/500, objective = 12152.805775465662.\n",
      "Iteration 160/500, objective = 11985.139819367314.\n",
      "Iteration 170/500, objective = 11832.954353307869.\n",
      "Iteration 180/500, objective = 11693.97286114733.\n",
      "Iteration 190/500, objective = 11566.362000839847.\n",
      "Iteration 200/500, objective = 11448.626819003945.\n",
      "Iteration 210/500, objective = 11339.5346441608.\n",
      "Iteration 220/500, objective = 11238.058841067397.\n",
      "Iteration 230/500, objective = 11143.336589108407.\n",
      "Iteration 240/500, objective = 11054.6367380189.\n",
      "Iteration 250/500, objective = 10971.335021385776.\n",
      "Iteration 260/500, objective = 10892.894721873068.\n",
      "Iteration 270/500, objective = 10818.851431389083.\n",
      "Iteration 280/500, objective = 10748.800926090882.\n",
      "Iteration 290/500, objective = 10682.389437824433.\n",
      "Iteration 300/500, objective = 10619.305787597154.\n",
      "Iteration 310/500, objective = 10559.274977804931.\n",
      "Iteration 320/500, objective = 10502.052935071977.\n",
      "Iteration 330/500, objective = 10447.422166120325.\n",
      "Iteration 340/500, objective = 10395.188142549987.\n",
      "Iteration 350/500, objective = 10345.176271520691.\n",
      "Iteration 360/500, objective = 10297.229341070053.\n",
      "Iteration 370/500, objective = 10251.205353206757.\n",
      "Iteration 380/500, objective = 10206.975676527087.\n",
      "Iteration 390/500, objective = 10164.42346422321.\n",
      "Iteration 400/500, objective = 10123.442294038792.\n",
      "Iteration 410/500, objective = 10083.934994902933.\n",
      "Iteration 420/500, objective = 10045.812631278739.\n",
      "Iteration 430/500, objective = 10008.993621226076.\n",
      "Iteration 440/500, objective = 9973.402968149729.\n",
      "Iteration 450/500, objective = 9938.97158943808.\n",
      "Iteration 460/500, objective = 9905.635727866058.\n",
      "Iteration 470/500, objective = 9873.336433868157.\n",
      "Iteration 480/500, objective = 9842.019108664006.\n",
      "Iteration 490/500, objective = 9811.633099797355.\n",
      "Iteration 500/500, objective = 9782.131341975046.\n",
      "Iteration completed after 500/500, objective = 9782.131341975046.\n",
      "Iteration 10/500, objective = 31377.43374741898.\n",
      "Iteration 20/500, objective = 22892.46823097318.\n",
      "Iteration 30/500, objective = 19452.03740402865.\n",
      "Iteration 40/500, objective = 17529.599145972956.\n",
      "Iteration 50/500, objective = 16278.039132543841.\n",
      "Iteration 60/500, objective = 15385.969656036496.\n",
      "Iteration 70/500, objective = 14710.907598425922.\n",
      "Iteration 80/500, objective = 14177.94267437706.\n",
      "Iteration 90/500, objective = 13743.685425756024.\n",
      "Iteration 100/500, objective = 13381.139886626881.\n",
      "Iteration 110/500, objective = 13072.558977642868.\n",
      "Iteration 120/500, objective = 12805.75469054018.\n",
      "Iteration 130/500, objective = 12572.054145842116.\n",
      "Iteration 140/500, objective = 12365.101462151846.\n",
      "Iteration 150/500, objective = 12180.121867506956.\n",
      "Iteration 160/500, objective = 12013.451662984033.\n",
      "Iteration 170/500, objective = 11862.22781879565.\n",
      "Iteration 180/500, objective = 11724.177018953917.\n",
      "Iteration 190/500, objective = 11597.46866531924.\n",
      "Iteration 200/500, objective = 11480.610175744132.\n",
      "Iteration 210/500, objective = 11372.370942411844.\n",
      "Iteration 220/500, objective = 11271.726137566206.\n",
      "Iteration 230/500, objective = 11177.814532997103.\n",
      "Iteration 240/500, objective = 11089.906389022097.\n",
      "Iteration 250/500, objective = 11007.378695205602.\n",
      "Iteration 260/500, objective = 10929.695858071991.\n",
      "Iteration 270/500, objective = 10856.394479978007.\n",
      "Iteration 280/500, objective = 10787.07124972346.\n",
      "Iteration 290/500, objective = 10721.373227008442.\n",
      "Iteration 300/500, objective = 10658.98998674204.\n",
      "Iteration 310/500, objective = 10599.647220278455.\n",
      "Iteration 320/500, objective = 10543.101485776604.\n",
      "Iteration 330/500, objective = 10489.135870401124.\n",
      "Iteration 340/500, objective = 10437.556380511114.\n",
      "Iteration 350/500, objective = 10388.18891703506.\n",
      "Iteration 360/500, objective = 10340.876724920701.\n",
      "Iteration 370/500, objective = 10295.478229892811.\n",
      "Iteration 380/500, objective = 10251.865194341568.\n",
      "Iteration 390/500, objective = 10209.921138245898.\n",
      "Iteration 400/500, objective = 10169.539981722659.\n",
      "Iteration 410/500, objective = 10130.624873961115.\n",
      "Iteration 420/500, objective = 10093.087179601529.\n",
      "Iteration 430/500, objective = 10056.845598589696.\n",
      "Iteration 440/500, objective = 10021.825399502417.\n",
      "Iteration 450/500, objective = 9987.957749577428.\n",
      "Iteration 460/500, objective = 9955.17912734699.\n",
      "Iteration 470/500, objective = 9923.430806006649.\n",
      "Iteration 480/500, objective = 9892.65839752.\n",
      "Iteration 490/500, objective = 9862.811449040853.\n",
      "Iteration 500/500, objective = 9833.843084550641.\n",
      "Iteration completed after 500/500, objective = 9833.843084550641.\n",
      "Iteration 10/500, objective = 31413.414999956716.\n",
      "Iteration 20/500, objective = 22962.70130626098.\n",
      "Iteration 30/500, objective = 19549.307253487703.\n",
      "Iteration 40/500, objective = 17649.576560048477.\n",
      "Iteration 50/500, objective = 16417.805129051918.\n",
      "Iteration 60/500, objective = 15543.406211945665.\n",
      "Iteration 70/500, objective = 14884.397154239978.\n",
      "Iteration 80/500, objective = 14366.203050136224.\n",
      "Iteration 90/500, objective = 13945.671271916244.\n",
      "Iteration 100/500, objective = 13595.979941339614.\n",
      "Iteration 110/500, objective = 13299.514070592986.\n",
      "Iteration 120/500, objective = 13044.18848463755.\n",
      "Iteration 130/500, objective = 12821.412073577965.\n",
      "Iteration 140/500, objective = 12624.89514191028.\n",
      "Iteration 150/500, objective = 12449.917305616118.\n",
      "Iteration 160/500, objective = 12292.86014432677.\n",
      "Iteration 170/500, objective = 12150.898752586125.\n",
      "Iteration 180/500, objective = 12021.792235875295.\n",
      "Iteration 190/500, objective = 11903.737811652833.\n",
      "Iteration 200/500, objective = 11795.266950044926.\n",
      "Iteration 210/500, objective = 11695.169988761834.\n",
      "Iteration 220/500, objective = 11602.440457380757.\n",
      "Iteration 230/500, objective = 11516.233311573304.\n",
      "Iteration 240/500, objective = 11435.833157769765.\n",
      "Iteration 250/500, objective = 11360.629768560959.\n",
      "Iteration 260/500, objective = 11290.0989972631.\n",
      "Iteration 270/500, objective = 11223.787745371299.\n",
      "Iteration 280/500, objective = 11161.302010447007.\n",
      "Iteration 290/500, objective = 11102.297301768138.\n",
      "Iteration 300/500, objective = 11046.470893928865.\n",
      "Iteration 310/500, objective = 10993.555519119142.\n",
      "Iteration 320/500, objective = 10943.31419361186.\n",
      "Iteration 330/500, objective = 10895.535944197223.\n",
      "Iteration 340/500, objective = 10850.032253295041.\n",
      "Iteration 350/500, objective = 10806.634081990347.\n",
      "Iteration 360/500, objective = 10765.1893613675.\n",
      "Iteration 370/500, objective = 10725.560866402562.\n",
      "Iteration 380/500, objective = 10687.624404906419.\n",
      "Iteration 390/500, objective = 10651.267267884754.\n",
      "Iteration 400/500, objective = 10616.386898253546.\n",
      "Iteration 410/500, objective = 10582.889742965735.\n",
      "Iteration 420/500, objective = 10550.690259889929.\n",
      "Iteration 430/500, objective = 10519.710055754176.\n",
      "Iteration 440/500, objective = 10489.877135429142.\n",
      "Iteration 450/500, objective = 10461.12524605515.\n",
      "Iteration 460/500, objective = 10433.393302171764.\n",
      "Iteration 470/500, objective = 10406.624880214544.\n",
      "Iteration 480/500, objective = 10380.767772587493.\n",
      "Iteration 490/500, objective = 10355.773593065847.\n",
      "Iteration 500/500, objective = 10331.597426572012.\n",
      "Iteration completed after 500/500, objective = 10331.597426572012.\n"
     ]
    }
   ],
   "source": [
    "ridge_accuracy, ridge_weight_matrix = model_validation_ridge([1,10,100], \\\n",
    "                                                         iterations = 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model on the validation data with the best weight matrix yields an 88.13% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9095"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(val_labels, multinomial_prediction_function(val_data_matrix,ridge_weight_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cells are testing the accuracy of the model on the classification of individual numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_data_matrix = polynomial_basis(val_data,degree = 1)\n",
    "y_pred = multinomial_prediction_function(ridge_data_matrix, ridge_weight_matrix)\n",
    "\n",
    "index = np.argsort(val_labels)\n",
    "val_labels_sort = val_labels[index]\n",
    "y_pred_sort = y_pred[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97.45547073791349,\n",
       " 96.54929577464789,\n",
       " 87.90123456790123,\n",
       " 87.23958333333334,\n",
       " 93.25744308231172,\n",
       " 82.70120259019427,\n",
       " 94.88817891373802,\n",
       " 93.33333333333333,\n",
       " 84.85639686684074,\n",
       " 89.0677966101695]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassifications = []\n",
    "for i in range(10):\n",
    "    index = (val_labels_sort == i)\n",
    "    a = y_pred_sort[index]\n",
    "    b = (a == val_labels_sort[index])\n",
    "    misclassifications.append(np.mean(b) * 100)\n",
    "    \n",
    "misclassifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model B: Multinomial Logistic Regression (no regularisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation_multinomial(traindata = train_data, trainlabels = train_labels,\\\n",
    "                           testdata = test_data,testlabels = test_labels, iterations = 1000):\n",
    "       \n",
    "    \n",
    "#---------------------------- TRAINING MODEL WITH THE TRAINING SET -------------------------------    \n",
    "    \n",
    "        \n",
    "    initial_weight_matrix = np.zeros((traindata.shape[1], trainlabels.shape[1]))\n",
    "\n",
    "    objective = lambda weight_matrix: multinomial_logistic_regression_cost_function(traindata,\\\n",
    "                                                                    weight_matrix, trainlabels) \n",
    "    gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(traindata, \\\n",
    "                                                                        weight_matrix, trainlabels)\n",
    "\n",
    "    optimal_weight_matrix = gradient_descent_mnist(objective, gradient, initial_weight_matrix, \\\n",
    "                                      step_size=1.9/(np.linalg.norm(traindata, 2) ** 2), \\\n",
    "                                            no_of_iterations= iterations, print_output=10)\n",
    "    \n",
    "\n",
    "#---------------------------- TESTING THE MODEL ON TEST LABELS -------------------------------\n",
    "\n",
    "\n",
    "    classifier = classification_accuracy(testlabels@np.arange(0,testlabels.shape[1]), \\\n",
    "                        multinomial_prediction_function(testdata, optimal_weight_matrix))\n",
    "\n",
    "        \n",
    "    return classifier, optimal_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/500, objective = 31373.43274279922.\n",
      "Iteration 20/500, objective = 22884.651931497465.\n",
      "Iteration 30/500, objective = 19441.20284857824.\n",
      "Iteration 40/500, objective = 17516.22372976163.\n",
      "Iteration 50/500, objective = 16262.444193709018.\n",
      "Iteration 60/500, objective = 15368.38797608562.\n",
      "Iteration 70/500, objective = 14691.516674196835.\n",
      "Iteration 80/500, objective = 14156.882984982389.\n",
      "Iteration 90/500, objective = 13721.071330344514.\n",
      "Iteration 100/500, objective = 13357.066558384706.\n",
      "Iteration 110/500, objective = 13047.107042301746.\n",
      "Iteration 120/500, objective = 12778.993456194252.\n",
      "Iteration 130/500, objective = 12544.043926942692.\n",
      "Iteration 140/500, objective = 12335.895298037181.\n",
      "Iteration 150/500, objective = 12149.766822842781.\n",
      "Iteration 160/500, objective = 11981.989831415085.\n",
      "Iteration 170/500, objective = 11829.697110860263.\n",
      "Iteration 180/500, objective = 11690.611789917988.\n",
      "Iteration 190/500, objective = 11562.900222068274.\n",
      "Iteration 200/500, objective = 11445.067190822547.\n",
      "Iteration 210/500, objective = 11335.87979573659.\n",
      "Iteration 220/500, objective = 11234.311201036571.\n",
      "Iteration 230/500, objective = 11139.49840944947.\n",
      "Iteration 240/500, objective = 11050.710114234429.\n",
      "Iteration 250/500, objective = 10967.321909661307.\n",
      "Iteration 260/500, objective = 10888.796953741454.\n",
      "Iteration 270/500, objective = 10814.670726316406.\n",
      "Iteration 280/500, objective = 10744.538902330929.\n",
      "Iteration 290/500, objective = 10678.047621826863.\n",
      "Iteration 300/500, objective = 10614.885622213911.\n",
      "Iteration 310/500, objective = 10554.777829495051.\n",
      "Iteration 320/500, objective = 10497.480100273306.\n",
      "Iteration 330/500, objective = 10442.774876918711.\n",
      "Iteration 340/500, objective = 10390.46757174681.\n",
      "Iteration 350/500, objective = 10340.383537182135.\n",
      "Iteration 360/500, objective = 10292.3655106161.\n",
      "Iteration 370/500, objective = 10246.27144709397.\n",
      "Iteration 380/500, objective = 10201.972671569116.\n",
      "Iteration 390/500, objective = 10159.352296586056.\n",
      "Iteration 400/500, objective = 10118.303861949995.\n",
      "Iteration 410/500, objective = 10078.730161104007.\n",
      "Iteration 420/500, objective = 10040.542225252615.\n",
      "Iteration 430/500, objective = 10003.65844122569.\n",
      "Iteration 440/500, objective = 9968.003783052125.\n",
      "Iteration 450/500, objective = 9933.509140442891.\n",
      "Iteration 460/500, objective = 9900.110730058383.\n",
      "Iteration 470/500, objective = 9867.749577659732.\n",
      "Iteration 480/500, objective = 9836.371061125888.\n",
      "Iteration 490/500, objective = 9805.92450589363.\n",
      "Iteration 500/500, objective = 9776.362825709386.\n",
      "Iteration completed after 500/500, objective = 9776.362825709386.\n"
     ]
    }
   ],
   "source": [
    "multinomial_acc, multinomial_weight_matrix = model_validation_multinomial(iterations = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Model on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9095"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(val_labels, multinomial_prediction_function(val_data_matrix,multinomial_weight_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_data_matrix = polynomial_basis(val_data,degree = 1)\n",
    "y_pred = multinomial_prediction_function(multi_data_matrix, multinomial_weight_matrix)\n",
    "\n",
    "index = np.argsort(val_labels)\n",
    "val_labels_sort = val_labels[index]\n",
    "y_pred_sort = y_pred[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97.45547073791349,\n",
       " 96.54929577464789,\n",
       " 87.90123456790123,\n",
       " 87.23958333333334,\n",
       " 93.25744308231172,\n",
       " 82.70120259019427,\n",
       " 94.88817891373802,\n",
       " 93.33333333333333,\n",
       " 84.85639686684074,\n",
       " 89.0677966101695]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassifications = []\n",
    "for i in range(10):\n",
    "    index = (val_labels_sort == i)\n",
    "    a = y_pred_sort[index]\n",
    "    b = (a == val_labels_sort[index])\n",
    "    misclassifications.append(np.mean(b) * 100)\n",
    "    \n",
    "misclassifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model C: Multinomial Logistic Regression (different degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_test_base = []\n",
    "deg_test_2 = polynomial_basis(test_data[:,1:],degree = 2)\n",
    "deg_test_4 = polynomial_basis(test_data[:,1:],degree = 4)\n",
    "deg_test_6 = polynomial_basis(test_data[:,1:],degree = 6) \n",
    "polynomial_test_base.append(deg_test_2)\n",
    "polynomial_test_base.append(deg_test_4)\n",
    "polynomial_test_base.append(deg_test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = list(zip(polynomial_base,polynomial_test_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation_multinomial_deg(traindata, testdata,\\\n",
    "                                     trainlabels = train_labels,\\\n",
    "                                     testlabels = test_labels,\\\n",
    "                                     iterations = 1000):\n",
    "    \n",
    " \n",
    "    \n",
    "#---------------------------- TRAINING EACH MODEL WITH THE TRAINING SET -------------------------------    \n",
    "    \n",
    "        \n",
    "    initial_weight_matrix = np.zeros((traindata.shape[1], trainlabels.shape[1]))\n",
    "\n",
    "    objective = lambda weight_matrix: multinomial_logistic_regression_cost_function(traindata,\\\n",
    "                                                                    weight_matrix, trainlabels) \n",
    "    gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(traindata, \\\n",
    "                                                                        weight_matrix, trainlabels)\n",
    "\n",
    "    optimal_weight_matrix = gradient_descent_mnist(objective, gradient, initial_weight_matrix, \\\n",
    "                                      step_size=1.9/(np.linalg.norm(traindata, 2) ** 2), \\\n",
    "                                            no_of_iterations= iterations, print_output=10)\n",
    "    \n",
    "\n",
    "#---------------------------- TESTING THE MODEL ON TEST LABELS -------------------------------\n",
    "\n",
    "\n",
    "    classifier = classification_accuracy(testlabels@np.arange(0,testlabels.shape[1]), \\\n",
    "                        multinomial_prediction_function(testdata, optimal_weight_matrix))\n",
    "    \n",
    "    print(classifier)\n",
    "\n",
    "        \n",
    "    return classifier, optimal_weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/500, objective = 76920.57694209095.\n",
      "Iteration 20/500, objective = 76532.29673809849.\n",
      "Iteration 30/500, objective = 76182.15869694277.\n",
      "Iteration 40/500, objective = 75858.49403888425.\n",
      "Iteration 50/500, objective = 75555.48914257948.\n",
      "Iteration 60/500, objective = 75269.3680826331.\n",
      "Iteration 70/500, objective = 74997.4319814007.\n",
      "Iteration 80/500, objective = 74737.66137717785.\n",
      "Iteration 90/500, objective = 74488.4963926648.\n",
      "Iteration 100/500, objective = 74248.70029482001.\n",
      "Iteration 110/500, objective = 74017.27062857692.\n",
      "Iteration 120/500, objective = 73793.37950656726.\n",
      "Iteration 130/500, objective = 73576.33237794714.\n",
      "Iteration 140/500, objective = 73365.53880408913.\n",
      "Iteration 150/500, objective = 73160.49122435319.\n",
      "Iteration 160/500, objective = 72960.74916990908.\n",
      "Iteration 170/500, objective = 72765.9272833842.\n",
      "Iteration 180/500, objective = 72575.68605738482.\n",
      "Iteration 190/500, objective = 72389.72455213801.\n",
      "Iteration 200/500, objective = 72207.77457659451.\n",
      "Iteration 210/500, objective = 72029.59596962892.\n",
      "Iteration 220/500, objective = 71854.97272440247.\n",
      "Iteration 230/500, objective = 71683.70977215149.\n",
      "Iteration 240/500, objective = 71515.63029051705.\n",
      "Iteration 250/500, objective = 71350.57343453153.\n",
      "Iteration 260/500, objective = 71188.39241153069.\n",
      "Iteration 270/500, objective = 71028.95283838687.\n",
      "Iteration 280/500, objective = 70872.13133228252.\n",
      "Iteration 290/500, objective = 70717.81429613693.\n",
      "Iteration 300/500, objective = 70565.89686727438.\n",
      "Iteration 310/500, objective = 70416.28200383976.\n",
      "Iteration 320/500, objective = 70268.87968798995.\n",
      "Iteration 330/500, objective = 70123.60622857229.\n",
      "Iteration 340/500, objective = 69980.38364885352.\n",
      "Iteration 350/500, objective = 69839.13914726216.\n",
      "Iteration 360/500, objective = 69699.80462097292.\n",
      "Iteration 370/500, objective = 69562.31624380093.\n",
      "Iteration 380/500, objective = 69426.61409110918.\n",
      "Iteration 390/500, objective = 69292.64180558.\n",
      "Iteration 400/500, objective = 69160.34629857923.\n",
      "Iteration 410/500, objective = 69029.67748260644.\n",
      "Iteration 420/500, objective = 68900.58803099285.\n",
      "Iteration 430/500, objective = 68773.03316152842.\n",
      "Iteration 440/500, objective = 68646.97044116788.\n",
      "Iteration 450/500, objective = 68522.35960937369.\n",
      "Iteration 460/500, objective = 68399.16241794557.\n",
      "Iteration 470/500, objective = 68277.34248552647.\n",
      "Iteration 480/500, objective = 68156.86516514591.\n",
      "Iteration 490/500, objective = 68037.69742344222.\n",
      "Iteration 500/500, objective = 67919.80773030694.\n",
      "Iteration completed after 500/500, objective = 67919.80773030694.\n",
      "0.64625\n",
      "Iteration 10/500, objective = 77319.5026205874.\n",
      "Iteration 20/500, objective = 77308.75741293613.\n",
      "Iteration 30/500, objective = 77301.63190258502.\n",
      "Iteration 40/500, objective = 77296.02089506965.\n",
      "Iteration 50/500, objective = 77291.31856392631.\n",
      "Iteration 60/500, objective = 77287.25089467755.\n",
      "Iteration 70/500, objective = 77283.6650953178.\n",
      "Iteration 80/500, objective = 77280.46228290946.\n",
      "Iteration 90/500, objective = 77277.571759389.\n",
      "Iteration 100/500, objective = 77274.93994750718.\n",
      "Iteration 110/500, objective = 77272.52492478126.\n",
      "Iteration 120/500, objective = 77270.29328332347.\n",
      "Iteration 130/500, objective = 77268.21808300994.\n",
      "Iteration 140/500, objective = 77266.27740165616.\n",
      "Iteration 150/500, objective = 77264.45325892998.\n",
      "Iteration 160/500, objective = 77262.73079777123.\n",
      "Iteration 170/500, objective = 77261.09765343399.\n",
      "Iteration 180/500, objective = 77259.54346316749.\n",
      "Iteration 190/500, objective = 77258.05948288561.\n",
      "Iteration 200/500, objective = 77256.63828581832.\n",
      "Iteration 210/500, objective = 77255.27352429563.\n",
      "Iteration 220/500, objective = 77253.95974037323.\n",
      "Iteration 230/500, objective = 77252.69221438262.\n",
      "Iteration 240/500, objective = 77251.46684307385.\n",
      "Iteration 250/500, objective = 77250.28004096413.\n",
      "Iteration 260/500, objective = 77249.12865998394.\n",
      "Iteration 270/500, objective = 77248.00992361429.\n",
      "Iteration 280/500, objective = 77246.92137260173.\n",
      "Iteration 290/500, objective = 77245.8608199306.\n",
      "Iteration 300/500, objective = 77244.82631328741.\n",
      "Iteration 310/500, objective = 77243.81610357622.\n",
      "Iteration 320/500, objective = 77242.82861840444.\n",
      "Iteration 330/500, objective = 77241.86243961295.\n",
      "Iteration 340/500, objective = 77240.91628417074.\n",
      "Iteration 350/500, objective = 77239.98898784467.\n",
      "Iteration 360/500, objective = 77239.07949118996.\n",
      "Iteration 370/500, objective = 77238.18682747375.\n",
      "Iteration 380/500, objective = 77237.31011225091.\n",
      "Iteration 390/500, objective = 77236.44853430545.\n",
      "Iteration 400/500, objective = 77235.60134777393.\n",
      "Iteration 410/500, objective = 77234.76786526643.\n",
      "Iteration 420/500, objective = 77233.94745184618.\n",
      "Iteration 430/500, objective = 77233.13951974372.\n",
      "Iteration 440/500, objective = 77232.34352370152.\n",
      "Iteration 450/500, objective = 77231.55895687216.\n",
      "Iteration 460/500, objective = 77230.78534718264.\n",
      "Iteration 470/500, objective = 77230.0222541222.\n",
      "Iteration 480/500, objective = 77229.26926587777.\n",
      "Iteration 490/500, objective = 77228.52599679271.\n",
      "Iteration 500/500, objective = 77227.79208509797.\n",
      "Iteration completed after 500/500, objective = 77227.79208509797.\n",
      "0.49701388888888887\n",
      "Iteration 10/500, objective = 77329.32638543147.\n",
      "Iteration 20/500, objective = 77323.983126283.\n",
      "Iteration 30/500, objective = 77321.18297334778.\n",
      "Iteration 40/500, objective = 77319.0621129215.\n",
      "Iteration 50/500, objective = 77317.25980453976.\n",
      "Iteration 60/500, objective = 77315.66357223141.\n",
      "Iteration 70/500, objective = 77314.22315633082.\n",
      "Iteration 80/500, objective = 77312.90889531761.\n",
      "Iteration 90/500, objective = 77311.69997425383.\n",
      "Iteration 100/500, objective = 77310.58046062854.\n",
      "Iteration 110/500, objective = 77309.53766339517.\n",
      "Iteration 120/500, objective = 77308.56127213976.\n",
      "Iteration 130/500, objective = 77307.64280908144.\n",
      "Iteration 140/500, objective = 77306.77524009517.\n",
      "Iteration 150/500, objective = 77305.95268466625.\n",
      "Iteration 160/500, objective = 77305.17019445893.\n",
      "Iteration 170/500, objective = 77304.42358156914.\n",
      "Iteration 180/500, objective = 77303.70928340609.\n",
      "Iteration 190/500, objective = 77303.02425492596.\n",
      "Iteration 200/500, objective = 77302.36588161858.\n",
      "Iteration 210/500, objective = 77301.73190853279.\n",
      "Iteration 220/500, objective = 77301.120381932.\n",
      "Iteration 230/500, objective = 77300.52960106284.\n",
      "Iteration 240/500, objective = 77299.95807814081.\n",
      "Iteration 250/500, objective = 77299.40450507434.\n",
      "Iteration 260/500, objective = 77298.86772575851.\n",
      "Iteration 270/500, objective = 77298.34671298326.\n",
      "Iteration 280/500, objective = 77297.84054917854.\n",
      "Iteration 290/500, objective = 77297.34841035603.\n",
      "Iteration 300/500, objective = 77296.86955269666.\n",
      "Iteration 310/500, objective = 77296.4033013498.\n",
      "Iteration 320/500, objective = 77295.94904106148.\n",
      "Iteration 330/500, objective = 77295.5062083202.\n",
      "Iteration 340/500, objective = 77295.07428476779.\n",
      "Iteration 350/500, objective = 77294.65279164823.\n",
      "Iteration 360/500, objective = 77294.24128512606.\n",
      "Iteration 370/500, objective = 77293.83935232107.\n",
      "Iteration 380/500, objective = 77293.4466079353.\n",
      "Iteration 390/500, objective = 77293.06269137484.\n",
      "Iteration 400/500, objective = 77292.68726428492.\n",
      "Iteration 410/500, objective = 77292.32000842028.\n",
      "Iteration 420/500, objective = 77291.96062380563.\n",
      "Iteration 430/500, objective = 77291.6088271317.\n",
      "Iteration 440/500, objective = 77291.2643503547.\n",
      "Iteration 450/500, objective = 77290.92693946103.\n",
      "Iteration 460/500, objective = 77290.59635337707.\n",
      "Iteration 470/500, objective = 77290.27236300014.\n",
      "Iteration 480/500, objective = 77289.95475033212.\n",
      "Iteration 490/500, objective = 77289.64330770494.\n",
      "Iteration 500/500, objective = 77289.3378370773.\n",
      "Iteration completed after 500/500, objective = 77289.3378370773.\n",
      "0.3534027777777778\n"
     ]
    }
   ],
   "source": [
    "classification = []\n",
    "optimal_weight_matrix = []\n",
    "for deg in degrees:\n",
    "    classification_acc, optimal_weights = model_validation_multinomial_deg(deg[0],deg[1], iterations = 500)\n",
    "    classification.append(classification_acc)\n",
    "    optimal_weight_matrix.append(optimal_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_classification = classification\n",
    "deg_optimal_weights = optimal_weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 1569)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a new polynomial basis for degree = 2\n",
    "\n",
    "new_matrix = val_data_matrix[:,1:]\n",
    "type(new_matrix)\n",
    "basis = polynomial_basis(val_data,degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6433333333333333"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(val_labels,multinomial_prediction_function(basis,deg_optimal_weights[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_data_matrix = polynomial_basis(val_data,degree = 2)\n",
    "y_pred = multinomial_prediction_function(deg_data_matrix, deg_optimal_weights[0])\n",
    "\n",
    "index = np.argsort(val_labels)\n",
    "val_labels_sort = val_labels[index]\n",
    "y_pred_sort = y_pred[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[93.21458863443596,\n",
       " 90.63380281690141,\n",
       " 62.79835390946502,\n",
       " 70.48611111111111,\n",
       " 31.78633975481611,\n",
       " 42.183163737280296,\n",
       " 86.5814696485623,\n",
       " 88.69918699186992,\n",
       " 28.37249782419495,\n",
       " 37.20338983050848]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassifications = []\n",
    "for i in range(10):\n",
    "    index = (val_labels_sort == i)\n",
    "    a = y_pred_sort[index]\n",
    "    b = (a == val_labels_sort[index])\n",
    "    misclassifications.append(np.mean(b) * 100)\n",
    "    \n",
    "misclassifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model D: LASSO Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation_LASSO(regularisation_parameters,traindata = train_data, trainlabels = train_labels,\\\n",
    "                           testdata = test_data,testlabels = test_labels, iterations = 1000):\n",
    "\n",
    "    \n",
    "#----------------------------- TEST RESULTS FOR EACH PARAMETER ---------------------------------    \n",
    "    \n",
    "    \n",
    "    parameters = []\n",
    "    classification_accuracies = []\n",
    "    optimal_weight_matrices = []\n",
    "    \n",
    "    \n",
    "    \n",
    "#---------------------------- TRAINING EACH MODEL WITH THE TRAINING SET -------------------------------    \n",
    "    \n",
    "    stepsize = 1.9/(np.linalg.norm(traindata, 2) ** 2)\n",
    "    for alpha in regularisation_parameters:\n",
    "        parameters.append(alpha)\n",
    "        \n",
    "        initial_weight_matrix = np.zeros((traindata.shape[1], trainlabels.shape[1]))\n",
    "        \n",
    "        objective = lambda weight_matrix: lasso_logistic_regression_cost_function(traindata,\\\n",
    "                                                                    weight_matrix, trainlabels, \\\n",
    "                                                                                   alpha)\n",
    "        gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(traindata,\\\n",
    "                                                                    weight_matrix, trainlabels)\n",
    "\n",
    "        proximal_map = lambda weights: soft_thresholding(weights, alpha * \\\n",
    "                                                 stepsize)\n",
    "\n",
    "\n",
    "        \n",
    "        optimal_weight_matrix, value = proximal_gradient_descent(objective, gradient, proximal_map, \\\n",
    "                                                                 initial_weight_matrix, \\\n",
    "                                                step_size=stepsize, \\\n",
    "                              no_of_iterations= iterations, print_output=10)\n",
    "        \n",
    "        optimal_weight_matrices.append(optimal_weight_matrix)\n",
    "        \n",
    "        \n",
    "#---------------------------- TESTING THE MODEL ON TEST LABELS -------------------------------\n",
    "        \n",
    "    \n",
    "        classifier = classification_accuracy(testlabels@np.arange(0,testlabels.shape[1]), \\\n",
    "                            multinomial_prediction_function(testdata, optimal_weight_matrix))\n",
    "        \n",
    "        print(classifier)\n",
    "            \n",
    "        classification_accuracies.append(classifier)\n",
    "        \n",
    "    return list(zip(parameters,classification_accuracies)), \\\n",
    "                                optimal_weight_matrices[np.argmax(classification_accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/500, objective = 31571.34703825616.\n",
      "Iteration 20/500, objective = 23102.63182116647.\n",
      "Iteration 30/500, objective = 19675.592711681875.\n",
      "Iteration 40/500, objective = 17764.224186264826.\n",
      "Iteration 50/500, objective = 16522.16380821228.\n",
      "Iteration 60/500, objective = 15638.608640999439.\n",
      "Iteration 70/500, objective = 14971.31058842001.\n",
      "Iteration 80/500, objective = 14445.50386308884.\n",
      "Iteration 90/500, objective = 14017.937821511956.\n",
      "Iteration 100/500, objective = 13661.575515851067.\n",
      "Iteration 110/500, objective = 13358.869539709096.\n",
      "Iteration 120/500, objective = 13097.643828602779.\n",
      "Iteration 130/500, objective = 12869.170689061091.\n",
      "Iteration 140/500, objective = 12667.159932659482.\n",
      "Iteration 150/500, objective = 12486.880467696781.\n",
      "Iteration 160/500, objective = 12324.701218186925.\n",
      "Iteration 170/500, objective = 12177.738116967785.\n",
      "Iteration 180/500, objective = 12043.761792539111.\n",
      "Iteration 190/500, objective = 11920.985509405815.\n",
      "Iteration 200/500, objective = 11807.94938435303.\n",
      "Iteration 210/500, objective = 11703.420780614715.\n",
      "Iteration 220/500, objective = 11606.369227334242.\n",
      "Iteration 230/500, objective = 11515.938172468748.\n",
      "Iteration 240/500, objective = 11431.375444759164.\n",
      "Iteration 250/500, objective = 11352.115684451186.\n",
      "Iteration 260/500, objective = 11277.630452043184.\n",
      "Iteration 270/500, objective = 11207.445537035184.\n",
      "Iteration 280/500, objective = 11141.158911131917.\n",
      "Iteration 290/500, objective = 11078.406253746123.\n",
      "Iteration 300/500, objective = 11018.920886575908.\n",
      "Iteration 310/500, objective = 10962.401712476258.\n",
      "Iteration 320/500, objective = 10908.617740637566.\n",
      "Iteration 330/500, objective = 10857.375008791067.\n",
      "Iteration 340/500, objective = 10808.461838879546.\n",
      "Iteration 350/500, objective = 10761.717759489318.\n",
      "Iteration 360/500, objective = 10716.978918200826.\n",
      "Iteration 370/500, objective = 10674.12261300909.\n",
      "Iteration 380/500, objective = 10633.011345471774.\n",
      "Iteration 390/500, objective = 10593.51165433884.\n",
      "Iteration 400/500, objective = 10555.527997671772.\n",
      "Iteration 410/500, objective = 10518.97155833378.\n",
      "Iteration 420/500, objective = 10483.767308597236.\n",
      "Iteration 430/500, objective = 10449.816926717445.\n",
      "Iteration 440/500, objective = 10417.05993338481.\n",
      "Iteration 450/500, objective = 10385.421454339556.\n",
      "Iteration 460/500, objective = 10354.845770555246.\n",
      "Iteration 470/500, objective = 10325.270007468262.\n",
      "Iteration 480/500, objective = 10296.638118876244.\n",
      "Iteration 490/500, objective = 10268.898771200516.\n",
      "Iteration 500/500, objective = 10242.014460547685.\n",
      "Iteration completed after 500/500, objective = 10242.014460547685.\n",
      "0.9060416666666666\n"
     ]
    }
   ],
   "source": [
    "lasso_accuracies, lasso_optimal_weight_matrix = model_validation_LASSO([1],\\\n",
    "                                                                      iterations = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9084166666666667"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_accuracy(val_labels,multinomial_prediction_function(val_data_matrix,lasso_optimal_weight_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing classification for each digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_data_matrix = polynomial_basis(val_data,degree = 1)\n",
    "y_pred = multinomial_prediction_function(lasso_data_matrix, lasso_optimal_weight_matrix)\n",
    "\n",
    "index = np.argsort(val_labels)\n",
    "val_labels_sort = val_labels[index]\n",
    "y_pred_sort = y_pred[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97.54028837998302,\n",
       " 96.54929577464789,\n",
       " 87.57201646090536,\n",
       " 86.89236111111111,\n",
       " 93.34500875656742,\n",
       " 82.88621646623497,\n",
       " 94.96805111821087,\n",
       " 93.33333333333333,\n",
       " 84.07310704960835,\n",
       " 88.98305084745762]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassifications = []\n",
    "for i in range(10):\n",
    "    index = (val_labels_sort == i)\n",
    "    a = y_pred_sort[index]\n",
    "    b = (a == val_labels_sort[index])\n",
    "    misclassifications.append(np.mean(b) * 100)\n",
    "    \n",
    "misclassifications"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
