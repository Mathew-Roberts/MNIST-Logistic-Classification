{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` and that you delete any **raise NotImplementedError()** once you have filled in your code. Enter your student identifier below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT ID = \"200878566\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdd840b9ab44a389a644d4ff41790a9a",
     "grade": false,
     "grade_id": "cell-6ded9e10fb35754f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# MTH786U/P final assessment template\n",
    "\n",
    "This is the coding template for the final assessment of MTH786U/P in 2020/2021.\n",
    "\n",
    "The goal of this assessment is to classify hand-written digits from the [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) and to present your results in a written report (at most 8 pages). The assessment is formed of three parts: 1) filling in the missing parts of this Jupyter notebook, 2) applying learned concepts from this notebook and the module MTH786 in general to the MNIST classification problem, and 3) presenting your results in a written report (written in $\\LaTeX$). \n",
    "\n",
    "Author: [Martin Benning](mailto:m.benning@qmul.ac.uk)\n",
    "\n",
    "Date: 18.11.2020\n",
    "\n",
    "Follow the instructions in this template in order to complete the first part of your assessment. Please only modify cells where you are instructed to do so. Failure to comply may result in unexpected errors that can lead to mark deductions. We load the Numpy and Matplotlib libraries. Please do not add any additional libraries here but at a later stage if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bd7074d1695e70e8c0a936b29704da2",
     "grade": false,
     "grade_id": "cell-a633ccf06b277795",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b8af802f1a0231178e470649ea44639",
     "grade": false,
     "grade_id": "cell-1521ab82136c8e27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Binary logistic regression\n",
    "\n",
    "For the first part of your final assessment you are required to implement the logistic regression model for binary classification problems as introduced in the lectures. Following up on what you have learned in the lectures and tutorials, complete the following tasks.\n",
    "\n",
    "Write a function ***logistic_function*** that takes an argument named _inputs_ and returns the output of the logistic function, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{1 + \\exp(-x)} \\, ,\n",
    "\\end{align*}\n",
    "\n",
    "applied to the input. Here $x$ is the mathematical notation for the argument _inputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7a31ab463bfd733a5550c57433d06c3",
     "grade": false,
     "grade_id": "logistic-function",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def logistic_function(inputs):\n",
    "    # YOUR CODE HERE\n",
    "    value = 1/(1+np.exp(-(inputs)))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d4000621fded3ac2c0b5634489af1d6",
     "grade": false,
     "grade_id": "cell-002e14c765e68b53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your function with the following unit tests. Passing this test will be awarded with **2 marks**. Please note that not all unit tests are visible to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ade774ac5cf141a9edbb4f46f707c70d",
     "grade": true,
     "grade_id": "logistic-function-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_almost_equal, assert_array_equal\n",
    "test_inputs = np.array([[0], [np.log(5)], [-3], [np.log(3)], [1]])\n",
    "assert_array_almost_equal(logistic_function(test_inputs), np.array([[1/2], [5/6], [0.04742587317756], \\\n",
    "                            [3/4], [np.exp(1)/(1 + np.exp(1))]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04ff095166cb1042179a534a57f7f5c4",
     "grade": false,
     "grade_id": "cell-8e2b052a692ae3ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the next exercise, write two functions that implement the objective function for binary logistic regression as well as its gradient, as defined in the lecture notes. The function for the objective function is named **binary_logistic_regression_cost_function** and should take the NumPy arrays _data_matrix_, _weights_ and _outputs_ as arguments. Here, _data_matrix_ is supposed to be a polynomial basis matrix, while _weights_ denotes the vector of weight parameters and _outputs_ is the vector of binary outputs (with values in $\\{0, 1\\}$). In order to generate a polynomial basis matrix, fill in the function **polynomial_basis**. You can follow the [solution](https://qmplus.qmul.ac.uk/mod/resource/view.php?id=1416413) of [Coursework 4](https://qmplus.qmul.ac.uk/pluginfile.php/2220881/mod_resource/content/4/coursework04.pdf) or use your own version, as long as it is consistent with the function header specified in the next cell and with the requested output. Subsequently, write a method **binary_logistic_regression_gradient** that takes the same inputs as **binary_logistic_regression_cost_function** and computes the gradient of the binary logistic regression cost function as defined in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32f2b5001d7a1881ffa12451d5133cb8",
     "grade": false,
     "grade_id": "logistic-cost-function",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def polynomial_basis(inputs, degree=1):\n",
    "    \n",
    "    basis_matrix = np.ones((len(inputs), 1))\n",
    "    for counter in range(1, degree + 1):\n",
    "        basis_matrix = np.c_[basis_matrix, np.power(inputs, counter)]\n",
    "    return basis_matrix\n",
    "   \n",
    "\n",
    "def binary_logistic_regression_cost_function(data_matrix, weights, outputs):\n",
    "    s = data_matrix.shape[0]\n",
    "    values = []\n",
    "    for i in range(s):\n",
    "        result = np.log(1 + np.exp(data_matrix[i,:]@weights)) - (outputs[i]*(data_matrix[i,:]@weights))\n",
    "        values.append(result)\n",
    "    return np.sum(values)\n",
    "    \n",
    "    \n",
    "def binary_logistic_regression_gradient(data_matrix, weights, outputs):\n",
    "    result = data_matrix.T @ (logistic_function(data_matrix@weights) - outputs)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "766d8951af9708ef7c8c6be80d721575",
     "grade": false,
     "grade_id": "cell-ee5c7e3b09c4f3f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After writing Python functions for the binary logistic regression cost function and its gradient, fill in the following notebook functions for the implementation of a gradient descent method. For the first function it is acceptable to follow the solution of Coursework 6, or to use your own version if is consistent with function header and output. For the second gradient descent function named **gradient_descent_v2**, modify the gradient descent method to include a stopping criterion that ensures that gradient descent stops once\n",
    "\n",
    "\\begin{align*}\n",
    "\\| \\nabla L(w^k) \\| \\leq \\text{tolerance}\n",
    "\\end{align*}\n",
    "\n",
    "is satisfied. Here $L$ and $w^k$ are the mathematical representations of the objective _objective_ and the weight vector _weights_, at iteration $k$. The parameter _tolerance_ is a non-negative threshold that controls the Euclidean norm of the gradient. The function **gradient_descent_v2** takes the arguments _objective_, _gradient_, _initial_weights_, _step_size_, _no_of_iterations_, _print_output_ and _tolerance_. The arguments _objective_ and _gradient_ are functions that can take (weight-)arrays as arguments and return the scalar value of the objective, respectively the array representation of the corresponding gradient. The argument _initial_weights_ specifies the initial value of the variable over which you iterate. The argument _step_size_ is the gradient descent step-size parameter, the argument _no_of_iterations_ specifies the maximum number of iterations, _print_output_ determines after how many iterations the function produces a text output and _tolerance_ controls the norm of the gradient as described in the equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a0df2f9342e9204a48960b5885f38d6",
     "grade": false,
     "grade_id": "gradient_descent",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(objective, gradient, initial_weights, step_size=1, no_of_iterations=100, print_output=10):\n",
    "    objective_values = []\n",
    "    weights = np.copy(initial_weights)\n",
    "    objective_values.append(objective(weights))\n",
    "    \n",
    "    for counter in range(no_of_iterations):\n",
    "        weights -= step_size * gradient(weights)\n",
    "        objective_values.append(objective(weights))\n",
    "        \n",
    "        if (counter + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=counter+1, \\\n",
    "                    m=no_of_iterations, o=objective_values[counter]))\n",
    "            \n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=counter + 1, \\\n",
    "                m=no_of_iterations, o=objective_values[counter]))\n",
    "    \n",
    "    return weights, objective_values\n",
    "\n",
    "    \n",
    "def gradient_descent_v2(objective, gradient, initial_weights, step_size=1, no_of_iterations=100, \\\n",
    "                        print_output=10, tolerance=1e-6):\n",
    "    \n",
    "    #This creates values for the cost function for each weight\n",
    "    objective_values = []\n",
    "    weights = np.copy(initial_weights)\n",
    "    objective_values.append(objective(weights))\n",
    "    iteration = 0\n",
    "    #This enforces a condition on the one norm of the gradient of the loss function\n",
    "    while np.sum(np.abs(gradient(weights))) < tolerance:\n",
    "        weights -= step_size * gradient(weights)\n",
    "        objective_values.append(objective(weights))\n",
    "        \n",
    "        iteration +=1\n",
    "        if (iteration + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}, objective = {o}.\".format(k=iteration+1, \\\n",
    "                     o=objective_values[iteration]))\n",
    "            \n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=iteration + 1, \\\n",
    "                 o=objective_values[iteration]))\n",
    "    \n",
    "    return weights, objective_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90e8d7a7ec95458b09266c3d92aaaec6",
     "grade": false,
     "grade_id": "cell-c3a6dd3a899dcb5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following cell, write a function **standardise** that standardises the columns of a two-dimensional NumPy array _data_matrix_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "924547dca886c6e5532970ba42bdb82c",
     "grade": false,
     "grade_id": "standardise",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def standardise(data_matrix):\n",
    "    row_of_means = np.mean(data_matrix,axis = 0)\n",
    "    standardised_matrix = data_matrix - row_of_means\n",
    "    row_of_stds = np.std(standardised_matrix, axis = 0)\n",
    "  \n",
    "    return (standardised_matrix/row_of_stds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33f3d6517199ac0dd790047f327a3631",
     "grade": false,
     "grade_id": "cell-f74f777a7e81f822",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your results with the following cell. A total of **3 marks** will be awarded if your function passes the following standard tests. Please note that not all tests are visible to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0ea50e31317e7e314d2a7d90e46ff41",
     "grade": true,
     "grade_id": "standardisation-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_matrix = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "assert_array_almost_equal(standardise(test_matrix), np.array([[-1.22474487, -1.22474487], \\\n",
    "                            [0, 0],[1.22474487, 1.22474487]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "faa82c0c746059f5e089fd2a8bc80f46",
     "grade": false,
     "grade_id": "cell-4ec03cc4fda49034",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To train a simple binary classifier, you require some data. The following cell calls a function that allows you to load the height-weight-gender dataset that you already know from your coursework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40ef881fba829b6964f8b1d65fcb6a47",
     "grade": false,
     "grade_id": "cell-d3175e8ccd63fab4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEQCAYAAABvBHmZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABOK0lEQVR4nO2de3xU9Znwv89MLhANEBEExICIogarC6i4tYq9SouXeqmt3XbVtWrX3W7b3bbaVl6Xtrvttt26u7Wr1nbt+1bQKrSordYbiu4CQrJSEhVFLiFyhwDhmmTmef/4nZmcOXPJSTKZTJLn+/mMkznzO2eeM4PnOc9dVBXDMAzDKCSRvhbAMAzDGHyY8jEMwzAKjikfwzAMo+CY8jEMwzAKjikfwzAMo+CU9LUAfc3xxx+vEydO7GsxDMMw+g21tbW7VHVUT44x6JXPxIkTWbVqVV+LYRiG0W8QkU09PYa53QzDMIyCY8rHMAzDKDimfAzDMIyCY8rHMAzDKDimfAzDMIyCY8rHMAzDKDimfAzDMAYZtZuauXfJOmo3NfeZDIO+zscwDGMwUbupmc8+uJzW9jhlJREevnkm0ydUFVwOs3wMwzAGEcvX76a1PU5coa09zvL1u/tEjoIrHxEZLyL/ISLLROSQiKiITAys+ZCI/FpE3hWRw97zf4rI6AzHGyIiPxSRrd7aZSJyUcFOyDAMox8xc9JIykoiRAVKSyLMnDSyT+ToC7fbZOBTQC3wCvDRDGtuA44FvgusB04F/hH4mIi8T1UP+Nb+AvgE8DVv7e3AH0XkAlV9vbdOwjAMoz8yfUIVD988k+XrdzNz0sg+cbkBSKHHaItIRFXj3t83Az8HTlbVjb41o1R1Z2C/i4CXgb9S1V96284GXgduUtX/8raVAA3AWlW9vDN5ZsyYodbbzTAMIzwiUquqM3pyjIK73RKKp5M1OzNsXuk9n+jbdjnQBjzq27cdeARnJZX3QFTDMAyjl+hPCQcXe89v+rbVABtU9VBgbQNQhnPxGYZhGEVGv1A+IlIJ3INTPL/zvXUckClRfY/v/UzHu0VEVonIqp07MxlZhmEYRm9S9MrHi+EswLnbPu251ZJvA5mCVpLrmKr6gKrOUNUZo0b1aB6SYRiG0Q2KWvmISAT4FfBh4EpV/VNgyR4yWzdVvvcNwzCMIqOolQ9wH3AdzuJ5IcP7DcDJIlIR2H4m0Aqs62X5DMMwjG5QtMpHRH4M3AzcqKq/y7LsCaAUuNa3XwlOYT2rqkd7W07DMAyj6/RJbzcRucb7c7r3PFtEdgI7VfVlEfkG8FXgl8A7IjLTt/tOVX0XQFVfF5FHgXtEpBTYAHwROBn4bCHOxTAMw+g6fdVY9LHA6595zy8Ds4DZ3uubvIefXwE3+F7fCHwP1w1hBLAauFRV6/ImrWEYRi9Ru6k5L90G8nWcQtEnykdVO8tGm9WFYx3GWUlf7aFYhmEYBSXYYXrunBqaD7V2WYEUS6fqrmAjFQzDMPoIf4fp1rY4cxfXE1ftsgLJ1Kl6+oSqoraGTPkYhmH0EYkO023tcUSEuGqaAkmQS5H4j5PoVF3s1pApH8MwjD7C32G6qqKMeU810NYeJxqN8N7ew9Ruak5aMLkUSaZO1fcuWZfRGioWTPkYhmF0g3y5tKZPqEruP2VMJYvqmnhs1WYeea2RRXVNSaXSmSLxHwcyW0PFhCkfwzCMLGRTMJ1ZIt1VTNMnVLF8/W7aYori4kCJ43RVkRTL3J5smPIxDMPIQDYFU7upmXuef5ujbXGUdEukp7GWqoqyZMPKuPe6u4okaA0VE6Z8DMMwMpDJ1QUkFYviWsQELZHOXGSdWUXNh1qJCMQVIuJeQ3Erku5gyscwDCMDmVxdfsUSEXj/5OP58odPCx1r8VtFJdEI10wfz9XTxverWE2+MOVjGIaRgWyuLr9iCCqeXPtBoK6nPc6CFR1JBYl1vRGrKcZ6H1M+hmEYWQi6usIqhmwusoRVk4gXZYoZ5dq/OxRrvU/RdrU2DMMoRqZPqOL2SyZ36wKeUF7Xn19NWVSIACJCVUVZ/gX1yBa76mvM8jEMY1DSV66ohFVTM254sp3O3U/U07BlH1cF4j/5kLlYY0ihlY83J+cC4CRgSPB9Vf1lHuUyDMPoNTpr6NkTxRR23+ZDrcl2Oq0xZf6KRhYG4j+5ZA7rPivWep9QykdEpgG/BcYDmTpSK272jmEYRtGTq6Hn3Dk1zHuqIfRF3q9sgKwKIqiUMsV/Wtvj3PP823z5w6cl5UysD9PlIBvFmKYd1vK5DzgAXAm8hRtRbRiG0S/J1tDzaFucR1c2ZoyRhOl08IFTR2UsPs1ktQB84NRRvPDmdtSrKo0rvPrOLlas3w0itMc61her+6y7hFU+ZwKfUtU/9KYwhmEYhWD6hCrmzqnh6fqt1Iwdxi//Z2OycLRhyz5KohFiMXeRr6ooy+qiC6ZOv/jWjmR3gmhEkt2lgx0RFtU1sbCuKbnNjwJtMWcL+ZXY7ZdMLkr3WXcJq3zeBo7pTUEMwzB6m4TrK9FBurU9zsqNe5h12iiee2O7c38pXDN9PCeOGJquYDK46PwWVCzuVIkA1844CcjcESHhYksoHvG2o0osrkQjAiJJBZiwcorRfdZdwiqfbwI/EJEVqtrYmwIZhmH0Bn7XV8RTFAnL4vjKcspLO1xawa4DZSURWtviIKTs13yoNeNIhNKSCFdNG5+iuAQ4a/xw5l5WA8CiuiY3PiEizJoymuMry5k6bnjSqoLMrr6BQijlo6rPiMgs4B0ReRtoTl+iF+dZNsMwjLzhVwTgrAtVTSqbqz1l4b/YJyylGy6YyIOvbkgqnoh09HTLNBIhYdHMnDSSaESIe12q39y6H0if43P3kx1Ka8EXUrsdDFTCZrvdAXwd2AnsB2K9KZRhGEa+CQbsg+nVQNZkAr+lFCFzT7cEC+uaaPXiOnPn1BCPd0R12mPKwrqmpJK7/ZLJfOu3a2htjwMk9xvISidBWLfbl4H7gb9RVVM8hmEUNZlqbRLWxsK6JgRnpeS6yPstJVVFxLnOSqIRTjquotN92trjPF2/NSWhQAQer21KyWLLlHAQ9pz6M2GVTwXwmCkewzCKnc46Rz++anOyqPPWiyZxx8fPSNk3cYFPWEodCsgpn1g8njJlNNiRuiQaSY7Cnj11LMvW7yYecypl+oQqVm1qTknjnjpueIr8wdfBcyqm/mw9IWxvt6dx3Q0MwzCKgtpNzdy7ZB21m1JD0IvqmjjSlto5+rMPLqd2U7NziXmKQIH7lq5n/gqXQzV/RSPX3b+MHz+7ls8+uByAh2+eyfsnH588tgKxOGk1QAlZ1m5rIVG0E4vH+dmSd2iPddgytY3NlEQjRMWlYm/Ze5j6LfuIeKX7/vk9foq1P1tPCGv53AM8JCIAz5CecICqrs+fWIZhDCZyjavOlASwqK6Jx1Ztpj2uKUWbC+uaeHRlakKuv1ZmV8vRtM9+dGUjU8ZUMndxPe1efOZoW5yFdU380yfPYvbUsfz3ul3EA/6wREPQbLGhWBya9h5J2ScWh6njKhk9bAgvv72TBa81IiJEIoLENWvx6EArMAUQ1WweRt8ikbjvZcYdVDWaL6EKyYwZM3TVqlV9LYZhDDoy1dwEx1Vn6gpw3QPLUqyJqMB151WzKEvRJkBZVLj78qnc/WRDMrifICJw1onDWd20L2W7CFxx9jieadjGkbb0fVSd9fLB00fz/JvbkwPmIiJJJZaJxJqEkkqcw6fPq05rLBps3VMsMR8RqVXVGT05RljL5yayx8EMwzC6RNBaSLS38bekCbqaFtY18dqGPSmKB5wCEFKLNtMQoWHLPtpj8bS34kqa4gGnXH73+pb0Q3nvKdAeV559YzvRiBBFiUYjzDptFHsPtfLaxg4H0YlVQxlaGmX9zgPJBAa/rHHveAl3Wjble/slk3N+r0GKOUkhbJ3PQ70sh2EYg4iUmhtV53Yi1e3kdzVFI8LjtU1pVgvArCmjASiJSLI7wKwpo9m+/wh/atrnlEQsTv17+xCRZEymO7hsN0lTgLG4ct7EKl7fvJfn39yeXKtANAIXnzaKqeOGJ4tQJWAdRTJkwfWkkSgUf5KCzfMxDCOvhLnbDltzk7gIb9l7mAWvpTdXEeDFtTuIxZRoVLjuvOpkl4CqijIatqyh3UsQ+FMG6ybteJ3opmPKoxzJoAABdrQcpT2uabGhWBwWrGikvLTjPFsOt3Hf0o4weaYsuJ7GeXqqvHqbrszzGQ18BphC+jwfVdW/yqdghmEUP0FFk+1uO7jO39hz9tSxXH9+dcbjJ9YmM9WSHQoczqpxG9pjyv9uauaR1xpdPCYq+PVEKHtHcyugA0ezV5tcWjOGB1/dQDzDzolebs2HWrn9ksncu2QdESEZJ5p8QiV/em9fiqLp6RyeYk9SCNvhYAqwHIjiGozuAo7zXjcDnd9SdBxrPPANYAZwNjAUOFlVNwbWVQE/xI1xGAosA76iqmsC64YA3wH+AhgBvA58Q1WXhpXJMIyuk0nRZEsJzpQ44G/smSj4zGY1JZTVXYvrc5omb25rSf4ddI2Fweu8k3SZhUGAK84Zxx0fP4Pqkcfw7d+uIZNtFFeS47KDiiFbe5+eNBIt1iFyCcJaPj8EXsMpgoPAbOBPwOeBfwQ+2YXPnAx8CqgFXgE+GlwgLqf7CeBk4G9xCu5OYImInKOqTb7lvwA+AXwNWA/cDvxRRC5Q1de7IJdhGF0gk6LJdLedTSGFUVL+C2bzodaUVjW9SVc+RYEnV2/hmPISasYNp8QrTA1aUEJHDU82xZBvBVHMXbDDKp9zgduARJJ8RFXbgV+KyPG4OqBLQh5rqaqeACAiN5NB+QCXAxcCH1TVJd7aZcAGXI+5L3nbzgauB25S1f/ytr0MNADzvOMYhpFnajc1897ewylzb3K5ikoiQltMkzNugGT3ABGh5XBbysybRJ1NsHtAqbdPoRDv0dknxtQVqUYjknS7CRCJdIxZKI1KiuurmBVDIQirfI4F9qhqXET2Acf73lsFzA37gaoa5l/O5cCWhOLx9tsnIk8CV+ApH29dG/Cob127iDwC3CEi5aqaXlVmGEa3SWlfExE+dMYJjKosT74fvKiu3dZCzEslxhWqM31CFTdcMJEHXllPe1y5b+n6FFeXAgtea2RYeQktR9tRXNuZa6aP5/XGZt7Y2uFe601Kvfqghi37eOS1RnJ58lxhqXqp35pMpGjY4jLugmMaBjthlc9GYIz391rgWlynA4A5wN68SgU1QH2G7Q3A50XkWFU94K3boKqHMqwrw7n4GvIsm2EMCvxFoP5MtBQ3Wkx5wSuwfGzVZhbcckFakeTcxfXJu3+/i80F5zs+L63BppKSEQbOmogmetEUAH86dBhXnALTqkdw6gmVaQWjRiphlc9zwEeAx4B/BR4RkQuBduB04Ht5lus4nMILssd7rgIOeOvSWv341h2X6eAicgtwC0B1deYsG8MYzMxf0ZhUGoq76JeXujhMVUVZUmkoJK2B1phy/8vvcvZJI5KKamFdU9oFvKqijOXrdycVUldIWBeJTLHeJq7w7d+tSRaVhmHlxmb+9N4+rpo2Pue6Yi4ALQRhlc+dQDmAqv5GRA4D1+G6Xf8b8PM8y5Ut2SR4yxN2XQqq+gDwALj2Ot0R0DAGKglrJag0ElbL65v3Zt33hTe38/yb2ykriXDDBRN5dOXmtDW//O8NVFWUdrtlileXWjD8Si4RA1KyK6NEzCpXXU2xF4AWgk6Vj4hEcdZNss+Eqj4JPNmLcu0hs9WS+HWafesymS5VvvcNw+gCy9fvTqtVieDcXas37+UFr4I/Ewkr6EhbPM1llmDdjgM5P78sKsnO08WC4CaXXjN9PALJTtjZSFh42Sj2AtBCEGakguKSCv6sl2Xx04CL5wQ5E2j04j2JdSeLSHCy05lAK7Cu90Q0jIFJIl06Ii5L7baLJvHp86tBhOfe2J7T3ZWPaEyxKR6AE0cMYdZpo7h62niumjae0mjuM/WnVWci8R1HfeO4BxudWj5ehttmXHFpoXgCuFFELlbVlwFEZBhwGTA/sO4fcQkQv/LWleBcgs9appthdB1/unQi2WD9roO0+Rp3Ci5xLaiIhpRGONxWuFToQtG09whNe4/w0ts7ufuyGq6dcRJvb2+htT3OBZNG0nK0nbe3t1DXuBeNK2WluRVKsMPDYLN6IHzM537gyyLye1XNrs5DIiLXeH9O955ni8hOYKenbJ7AdTT4tYh8jY4iUwH+JXEcVX1dRB4F7hGRUlwd0Bdxxamf7amchjFYyNT+BjqKPv1KpjQqXDvjJIaVl/DgqxtSYkMDQfHkSmZobY9z1+J61OvCLcDa7S1Z2whlo3ZTc8YOD4OJsMqnEjgFWC8izwBbSY23qar+ny587mOB1z/znl8GZnnW1hzgR957Q3DK6BJVDUYwb8Rl230X115nNXCpqtZ1QR7DGLRkC34vzDIfZ3RlOcPKS/j5K+tz1r30VzrLootlScToivLIFvMZTBlwYZXPN31/35ThfQVCKx9V7dQ1rKp7vM/K9Hn+dYeBr3oPwzC6iH8I29G2OIvqXPeqx1ZtzpjR9d7eI1mTCQYbETpiNl3JYMvUhmiwZcCFnecTJjHBMIx+Ru2m5hQlozilA66ANCxRYUBaQdmIRoQvXHgylUNLk1bKvUvWhc5gy9SGqCv7DwRsno9hDGIW1jWlKZn2uLKzpWu5OoNJ8QDE40rL0Xbu+PgZyW1dHWEQbENU7CMQ8o0pH8MoQnrq+880Z2dRXVNKj7H5Kxp5dGW6ay2usHlPsGNVKqOOLeNga4xDrdnn2wxkEhaiv4VOT0cYFPsIhHwTdp5PnNwFvfuAOuCHqvpsnmQzjEFJT33/wf3nzqnh7icbkt2gf7OykXlXnMVdv1uT1WJZuz13486dB3qc9NrvicU1xTWWj2SBwdTpOmws5zvAZmAn8BDwA1xdzU6gCfh/wCjgaS9LzTCMbpJt/k1393+6fmvKGIL2OPzz02/kdJUVaGxO0THq2NSuBGeMqUxbUxaVtOLQhML/8bNr+eyDy6ndlKnlpOEnrNvtCK6GZraqHklsFJGhwNM4JTQN+D0uM+6pPMtpGIOGnvr+g/vPnjqW/3k3tZFny5FUd9l5E6tYtbG507k1A51RleXsOdhKTKEkAuOPq0iZjgpw7YyTGDdiaIqFY+1yuk5Y5XMb8Ld+xQMuzVlEfgL8VFW/JyIP4nUaMAyje2Tz/WcbcZCJq6eNT4nvvLZhN797fUvGtaVR4co/G8//bt5LfLBlDgTwzwlqj7si0pKoJEdyl0Ul46iEwZYskA/CKp/RQGmW98qAxDe9i/y0dzKMQU3Q9++P4yQMmJKIMO+KqVx/fnXKus88sMxNDfX1H9t9MHuMRlV5ae2OLqVW93f87fCztcYHZwk9essFackaQQZbskA+CKt8VgF3i8j/qOrWxEYRGYcrLl3lbZqAr/u1YRi5CRuk9rt1ErTHlbmL61Nasyysa0o25myPKfNXNPLYqs1UlEWzHrs9Ds++kb1T9UAjeHecTfGURiX53vc+eVanxx1MyQL5IKzy+TvgBWCDiCwDduCsoQuAQ8BfeOsmk9r40zCMLHSnIj6ogOKq3Pfyuxxpi1EzdhgN7+1L27ctpuw73N5bp9EviUYkpScdwORRx3D+pJHUjBtOw5Z9PLZqM4+81siiuqYB322gLwiV7eb1SZuMm2IaB87ynn8MnKqqr3vr5naxx5thDFq6ktWW6IL8/snHc+U54yiJiLuDV3juje288s4u7lu6nj81pSsfI5VIBD54+mhOrBqasn3SqGP53ifP4vrzqxk3Yijtce12xqHROaGLTFV1N6k93gzD6AFdCVL7uyCXlUSY876x/O71LWkuo2QcwwtkDJ4oTnhinpsxkiM6bQkEvU+XOhyIyHE4V9txuOSCFV4DUMMwukimIHW2GJDfSjraFmdxlsy1BIUcM91fCdYyKXDvknXJ794SCHqX0MpHRL4L/D1Q7tt8VER+pKp35V0ywxgE+IPUiRjQ0bY4InDLByYle4dVVZQlL5aZ9EpUoKwkfZCbyOBVRGeOrWTttpZQfedKIvDy2zt54c3tKfE3Uzq9R9j2Ol/Gudx+Afwa2AaMwSUafFNEdqrqv/eWkIYxGFi+fndytIEq3Ld0PdUjj2HKmEoeXdmYXJcpNTimmQe5DVbFA65m57aLJrH/aDvrtrewcmNz2vcmwIWnHk/1cRUseK3RikQLSFeKTP9NVb/i27YWeFlEDgB/DZjyMYweMHPSyDRL5dGVjby5rSWlPY6SuzbF6ODJNVu5/jxXB7VqU3OaMlZg9tSxAES8QJnFeApDWOUzEdc6JxO/x42uNgyjEzqr65kxoYrXNnb0BTvYGktRPAlM8YTjvebD/PjZtZREI5REhFhcEelIs44A9Vv2saiuiVhciUaEuXNqzOopAGGVz25gKvB8hvdqvPcNw8hBproeINky5+4n6l1nggiMGTaErfuOsG7HgT6WuriZcFwFmzoZ/5BwpX3kzBM4+6QRVFWUMe+phmQmmwCt7Ql3p9J8yDp2F4Kwyue3wHdEZDfwiKq2iUgJcC0wD+vnZhid4s9Ya21346oX+kZYJ4jFYcu+I4M6XhOGi049nqXv7Aq1VoGX1u7g1otPYfqEKqaMqUxaoOAN1bO06oISVvncCZyNUzK/FJE9uHTrKPAqVv9jGGkEG4G2HG5LZqzFFXa0HE1TPAlM8eRmwnEVad/bsWVRDviG2507sYoRFWU857UOao91zN8JZrJZWnXhCaV8VLVFRC4CPgF8AKd49gAvA0+r2v8qhuFn/opG5i6u74gtSCKg7YgAoyvLM7Z5MTqn5UgbLYfbUrb5FY8As6aMpsqnfOLA6s17qd3UnKZgLK268HSlw4Hi5vTYrB7DyEHtpuYUxQPO0lF1AW3UZVRdNW08NeOG55woamRmz6E29hzK3kooGhFmThrJ8vW7iUhHQelzb2xn6Ts7rVdbEdClDgeGYXTO8vW7iWdwBqj336knDucC78L4zvYWRlWWU14SpbH5kLnb8kBEYN4VU5PKpawk0lE/hdXxFAtZlY+IxAmf0amqaorMMEjtQB0R4c+qR7DSS5+OxWF10z5WWwPQXiEq8J0rz0rOOEq0yVlU18RjqzYTi1sdT7GQS2HMw8oJjAFK2Dk6YY7hnywKsKiuiQ+cOorRleVcNW08a7e1JJVPV4gKg94dl62YdmhphCknVPJ6QIl/+rzqlOF60BHPuWraeEsqKCKyKh9VvbuAchhGwejKHJ1sJBIKYnFNdhyIRlxsIRFfiApUlpfQsHV/tzoSDHbFA/DhM0/ghTe3pzQBjQrcNaeGp+u3pqwtibgR19mwpILiwlxlxqAj0xydrlyUMiUUKG4iqJ+Y158toXgigESc683onKjAjv1H0rpPX3b2OOY91cBRr5edU/ySEucxip9cMZ+5wIOqusX7Oxeqqt/Jr2iG0Tv0dFbL8vW7iXUhPTphGY0eVs6x5SWs23mwawIPVkRYE5jMKsDug63JjgQR4P2nHs+XP3yaKZ5+Ri7L527gGWCL93cuFDDlY/QLejqrZeakkWn1OWOGlbOj5Shx7YhTBJ+37T8KHM3beQx0Egre77IsjQqzp45l5cY9yZuH2VPHJieNmgLqP0ix1oeKyPuB/wOcAwwB1gE/VdVf+tZUAT8ErgSGAsuAr6jqmrCfM2PGDF21alX+BDcGBcGYTyZKIsLNF57M82/tsB5tIRlRUcreQx3Fo8FYWWlUeOSWC4COnnj+Ca9Wv1MYRKRWVWf05BiRkB9U1sn7Y3siRIbjvQ/XxLQU+AJwNbAS+IWIfNFbI8ATwKXA33prSoElIpI96mgYeeD686t59NYLeN/44VnXxOLKMw3b2H3gSAEl61+MOjb10pJQPBGgLCpp36+/Rc7tl0ym+VBrWvzO6B+EUj7AfO9in4aneJbkTyQAPo3rG3eZqi5W1edU9VZgBfB5b83lwIXA51R1gao+422LAF/PszzGIKd2UzP3LllH7abwKdMKbNx9iOZD7b0nWD9n+NDStG2Ci+MsuOUCrju3mqjvylNaEqGqoiz5WyTid1HB6nf6GWGz3S4Efgrc7t8oImNwiiffjuwyoA04HNi+F0jY1JcDW1Q1qfhUdZ+IPAlcAXwpzzIZg5RgavbcOTU0bNnHI681Zk2HLisRWtuL06VdaLKlmUeASaOOTUvA8A94m/dUA4rLZvvQ6aOZNWV0mpvNmoL2T8JaPp8APici/yexQUROAF4E2oEP5Vmuh7znfxeRcSIyQkS+4H3OT7z3aoD6DPs2ANUicmyeZTIGKYnx1nGFo21x5i6u5+EV2RUPYIrHR7ZvQiJwfGU5t100iTHDylPea9iyLyUlHlXOPmlERjdbwgVniqd/Ebarda2IXAM8KSJbgUU4xSPAh1Q13FCNkKhqvYjMws0R+mtvcxtwm6o+4r0+DtiYYfc93nMVYFHeAUo+OhSEOW7tpmZWb96bvIAqdCnN2nAkPGfBuUXzVzRSEoFhAfebkjklfu22FiIiqLqJpFUVOcPRRhHTla7Wz4rITcB/AXcArcAsVd2eb6FE5FRgIc6KuQ3nfrsCuE9Ejqjqw2S35jPGpgLHvwW4BaC6urqT1UaxkY8OBWGOO3dODfOeauBIW2pVqKmerhPJ0SqoPQ57DnZkuJVFhaunjU9LiQfnhkukuMfiyrynGpgyptKsnn5IriLTSRk2LwPuB67DZZdVJNap6vo8yvVPOEtnjqom/lW+ICIjgX8TkQU4C+e4DPsm/hVmjQyr6gPAA+BSrfMmtVEQetqhIMxxj7bFeXRlI63BtgVZKLG5PFkZM6ycXQdbQ/ULmjiygh9/6pzk7+lviXPvknUpv4d1qO7f5LJ81pHDXQu8FNgWzYdAHmcBq32KJ8FrwPXAaJxV9NEM+54JNKqqudwGKD3pUJDLXTdz0khKopFk9XzDln2URCO0x+LJFi/ZzG1TPNmpPq6CHS0uJ8nvfotGYFp1VUrT1VsuOiWrIkl2C2+LE8dZU5bh1n/JpXxuLJgU6WwDzhGRMlVt9W0/HziCs3qeAG4UkYtV9WUAERkGXAbML7TARuHoboeCztx10ydUcc308SxY0YgX4+aa6eM5ccRQqirKaNiyj4dXNPbSWfV/SiKgSFpMbHhFWcrNwtw5Ncku4NMnVDF/RSNP129l9tSxaR2p/fh/d38ncbN6+ie5ulr/qpCCBPgp8BguweFnuJjP5cBngJ+oaquIPIFzA/5aRL6Gc7Pdibu5+pe+EdsoFN3pUJzLXVe7qZlFdU2s296CN2yU0pIIU8cNp37LPrbsPcw721t641QGBB898wRuvfgUAH7w9Ju85rNmXn57J3dfVpNVWVx/fvoYhGxYZ+qBQ1F2tVbVx0Xk48A3gAdx7XXexdUZ3e+tiYvIHOBHwM+8NcuAS1R1c58IbvQ5nbnVMrnrvv+HN7n/lfUpU0QFuLRmDHMXr0nrVm2kIsDZJ41Ift+/ue3P+dZv1zDfsyBjsTjNh1q5/ZLJfSqnUVwUpfIBUNWngac7WbMHuMl7GIOcMG61oLtu/opG7luaniujwO9e31JA6fsvEYHVm/cmuz8sX7+bmnHDKS/tfudwY+BTtMrHMLpKMFttYV1TmvUTdNs8utJiOD0lpvDsG9t58a3tRCIuQaMsQ2zHMPyY8jEGDDMnjaQkIrTGXKfpx2ubuNqbbJnJFTd/RSOb9xwKdezuTCIdbLTHQeLxZAq0udqMXJjyMQYM0ydUMWvKaJ59w9U9x2JxFtU1sbCuKaVwtH7LPtZtb0kJiudi2JASRleW2xC4TogA0ajQHlOiUXO1Gbkx5WMMGGo3NfPS2h3J15GIUP/ePo62ubvx1rY4d/1uTZhaxxT2H2ln/xHrTJ1gwnEVVI+sYGhplOfe2N5hEUqHdRiLxVm7rcXcbUZWQisfr5PBp4BqXGaZH1XVv8qnYIbRFWo3NXPP828niz0FiMeVNe/tS04TlRwtXozMRDN8Z1PGVDKq0k1ujUQ66npU3bwdcPvMXez6/maL+/RWfz6jfxBK+YjIFbi6mwiwg/QRCva/tJF3wl6cElluCQsnIhCR1HY3iv0j7Q6ZlPULb+1IKpyouHEHGldKokJMOxqvxuLKXYvriceV0pIIC74wM6Wuqjf68xn9h7CWz3dx7XQ+q6o7e08cw3B05eKUyHJLXCcryqJMq65i6TupzdatA05+8HcwiCt85ryTOHHE0GTX6bmewhGfVdTa7uJvid+wt/rzGf2HsMpnEvD3pniMQtGVi5O/JxvAgaOxNMVjhOPY8igHjsZStuXK9Cv1daAGl/QxZUwly9fv5qW1O1L6tvmP0ZP+fMbAIOwwubcA+9dhFIyujEdO9GTrjLD/2AczQcUTAd43fjjnTUxV/CLwkTNPYMEtF6S40u5dsg5wv9/qpn3J9SURkmnv0FHw+9WPTjGX2yAlrOXzdeAeEVmR59EJhpGRrjYPvXraeH6zsjGtFU7Eu21XQK1YJyuVQ6K0HImlbY8Dq5v2URqVFAtIgHN8LXWCbtKrp42nPRZPrr3u3OpOC36NwUXYm8G7cZbPmyJSLyJLA4+Xe09EYyCRuDtOtGLJRWI8MtDpPtMnVPHorX/OR848gTHDyjta92tHsoGa4snKgQyKx0+bV7ibIBqRFGs06CZVSFqu5aURrprWuWVqDC7CWj4xYG1vCmIMfLqT4dSVfaZPqOLnn5+R3KetPU40GiEWi1uKdSd09es5c+ywlN8hGMO5etp4rp423lKpjayEUj6qOquX5TAGIMFU6c6SCDKlVgf3WVjXlLImuE/i9Q0XTGTZ+t2MHjaEN7fso2nvkb76GgYEQY/ldeemjkDI5iY1pWNkwzocGL1CJoslV4bT/BWNLkVXNcXC8e8TjQiP1zYlG1fecMFEHnx1A7G4UhoVzjlpBKs2NQdSqvelyWZ0nc+cX83UccNzDn2zGI7RFbIqHxG5CKhT1QPe3zlR1aV5lczo12Sycm6/ZHLa3XFiiNsjrzUmXWOtPqvIf0e9evPeZDuX1vY4D7yyPqloWmMaulebER7BxWwS6dRhh74ZRmfksnxeAmYCr3l/Z3MLJyzyaD4FM/o32awc/91x7aZmPvPAMloDARkR4b29h6nd1Jyy/t+ef7sj20qEuFWN9hoTR1Zwac0YGrbuZ/bUsZ12mLDYjtFVcimfS4A3fH8bRmjCpEovrGtKUzxRAVAWrGjk8VWbk3Uki+qaaIt19G370OmjeWntjrT9O6Mkktp2x8jMpTVjeGjZRlrb46zcuIcpYyoz/obWJsfoLlmVj6q+nOlvwwhLZzEACbw+e/xwThg2JDkSoTWm3Pfyu4yuLOfRlY0ppvf6XQc556QRXXa1lZVEaG/NnVY8GCmJCDdfeHLS0mk+1Bqqw4S1yTG6iyUcGH3GVdPG81htk5cSLUw9cTg7W1J71r7w5va0nmwKrNtxoFufecgUTxpRgXlXTE2J59Ruag7V/sba5BjdRXSQV97NmDFDV61a1ddiDEjCxAJqNzWzsK4pmcVWEo0Qi8eJxTMuN7pBrsYOAlx/fjXf++RZKdsTv4vgbhIs5mP4EZFaVZ3Rk2OY5WP0CmFjAYn6n/aYc93EYnGmnjicPzXts044eWLciCFs3XeEuEI0Ah86/QReensnsZizVoLdB4K/XWfdCSzF2ugO1mvR6BUyxQKyEWwiet251UTtX2be2LL3SNJ1qQpnnzSCBV/I3tSzK7+dYXQXs3yMXqErsYBgZhyA5nQWGWEZU1nOtkAcLeEeyzWiwuI4Rm8TdpLpi8Bfq+pbGd47DbhPVT+Yb+GM/ktXu1JPn1DF2m0t3PP82wwtjTLYY5H5Yt+RtpTXk44/JtRv0ZXfzjC6Q1jLZxYwLMt7lcDFeZHG6PcEg89hL1zf/8Ob3Le0Y1pHMA3byE1E3FTRiLhZO4mEjcNtqZkbN104KdTxLI5j9DZdcbtluxU9Behe3qsxoOhqwWGitc6OlqM8/+b2lPfM7gnPiKEl7DvcDjilPXXc8JRBbontt140ydrjGEVDrt5uNwI3ei8VeEBEWgLLhgJTgRd6RzyjmMiVUlu7qZl7nn+bo21ulsvRNteBOlvXaoBP3f8/llLdTRKWYTQCB1pjSWUdjbqEjTe3NSTHioOzhiqHlhZcTsPIRi7LJ46b4wPu37r/dYLdwH8CP8i/aEYxkcuq8b+XuAgq8HhtU7IhZXD/s04cboqnhySsmAWvNQLuf9Jrpo9nyphKrp0+nne2t7BqUzOqUBK1xAGjuMjVXudXwK8ARGQJ8MVMCQfG4CBXGxX/e/4ctVisY51/zdG2OA1b9qd9xnkTq6wzdUgUaNxziJqxwzrSqIFh5SVJJV8SESIRIRZTG+NqFB2hqilU9RJTPIMbfy1ONBpJdp0OvldaEqEsKsm/E3fbVRVlHaOtydzm5mi7mUK5qCiNpCRi/Pe6XTz46obk6wjQsHV/x01CTIl5469jcbV6HaOoCJ1wICLDgI8D1cCQwNuqqt/Jp2DeZ34cuAOYhnP7vQ18XVVf9N6vAn4IXImLPy0DvqKqa/Ity2AnkX67qK6Jx1Zt5pHXGllU15R0vwXrdJav301VRRnL1+9m7bYW7n6ivtNR1pt2HyzAmfRfDrXFKYkKNWOHsea9fZ7Fo5REBFWltCTC7KljWblxT3L4HiLJTgbmdjOKibB1Pu8HngRGZFmiQF6Vj4jcCvzUe3wHd2N3DlDhvS/AE8DJwN8CzcCdwBIROUdVm/Ipj+FrhRPXNPdbptTchPsn7ASDvV7GlpGd9phy8Gi764HnKZW5c2poPtSaTASZMqYy7UbA6nWMYiOs5XMPsBH4ArBGVVt7SyAAEZnofebXVPUe31t/9P19OXAh8EFVXeLttwzYAHwd+FJvyjhYCY613rL3MPNXNKZc/IKZb0Z+WbfzICVR4dPnVWds+hm8ETClYxQjYZXPGcCnVLW2N4XxcRPOzXZfjjWXA1sSigdAVfeJyJPAFZjy6RUSLrZEJ+r5K9ycnYi4WTlz59Qw76kGUzx55KJTj2f5hj0pqdPtng/TFIvRXwnbvrERKO9NQQJcCLwFfFpE3hWRdhFZJyK3+9bUAPUZ9m0AqkXk2EIIOpCo3dTMvUvWJRMJsr2/dlsLm/ccos2XWh1XONIW599fMIsn3yx9Zxc3/flEJo86JmX7jkDPNsPoT4S1fP4RuENEXlDV9BzZ/DPOe/wQ+CbwLnAt8FMRKVHVfwOOw7kCg+zxnqvI0nlBRG4BbgGorraKb+i8O8H8FY3MXVxPLO6yp7K1/dy23y6IvUHD1v2cP2kk63Z2JGWMrizk/aBh5JdcHQ7+b2DTCcAGL66yJ/Cequpf5lGuCK5n3A2qusjb9qIXC7pTRP6d7Ne/TtuCqeoDwAPghsnlReJ+zqK6pqTFEqzjmb+ikW//bk1K4oD/S7P+071PzdhhfKRmTHLya6Y5PIbRn8hl+VxE6jVFgf04d1eQfF97dgOnAs8Ftj8LXAqMxSnA4zLsm7hdt2rFkNRuauaxVZtTWrQkMqVqNzUzd3F9zow1Uzw9J9EYNBMCPLRsIx+pGcOCL1i3aWNgkKvDwcQCyhGkAZiZYXvCqol7az6aYc2ZQKOqWrPTkCTSp6GjRYu/e0HMd1WMiAtyr9rYbEonDyT+QZdEhOqRx7Buh/tnGwGqR1awafchFGj1rNHbL5lsSscYEBTrvMjfes8fC2z/GNCkqttwNT4nikhynINXCHuZ956Bs1y++ds1fOu3a7ImEvg7FJSXRrja586ZOWkk5aURIrgL5HevPIs7Zp9BaUmx/tMpbqIRp8DBKR4RZzm2xZRJxx/DkFL3O5SVRri0ZkxKQkdVRVkfSW0Y+SdskWmuqHwc2KeqwY7XPeEPwBLgfhE5HlgPXIOzdBKdtp/AdTT4tYh8jY4iUwH+JY+y9FtqNzXzmQeW0eql5T5W28SCL6SPOcg1PGz6hCrmzqnh6fqtzJ46liljKrnv5Xdps66gXUKAaET481NGsvSdXQDJxI3E3y+t3cFN7z+Zhq37mT11LA1bUsciBF8bRn8mbLbbRjpx7YvIeuBfVPXnPRVKVVVErgT+GZdpV4VLvf6sqs731sRFZA7wI+BnuJY/y4BLVHVzT2UYCCxfv5s2X0+bYCKBn2zDw2o3NXP3E/W0xZRl63eDKtaCLTzlJRGOeinpcVX+9F6qAqkoi3LgqOtz1xpTHnjFDdRbuXEPHzh1VMpac3MaA4mwyuc2XMrzXmAhsB0YA1wNDMdd/C8C7hORNlV9qKeCeSndt3uPbGv24ApSb+rp5w1EZk4aSWlUkpZPd/p7LaxrSu7f3llzNiMNf7PUuML7ThyetHwAplVXpbxOhNda2+POWvISEUqjkuIONYz+TljlcxqwSlWvCWyfJyILgTGqOkdE/h/wd8BDeZTR6CbTJ1Sx4JYLWFjXhEDGViyQe0icjbPOHxHg/EkjOXPsMJ5p2MalNWOoHFrKq+t2pWW6iQgvrd2B4tx1d18+1RINjAFFWOXzF8ANWd57EKdsvgo8hrOGjCIhmzstQbC4NNik8qpp41NqS84cU8nrTRZ76A4lUUl+r3d8/AzAff9lJZGUeUjRiPDB00fz/JvbvW1K86FebadoGAUnrPKpBEZleW8UkGhls5/0aadGEeMf8tbaFueuxfXE40pJVLh2xklcPW18srak5XAb9y1d39ci91uqRx7D2m0tKVamP9mjqqIsqfgBlr6zM6n0bRyCMdAQDTHhUET+AJwFXOlvLioiM3Bp0as9t9sXgC+raqZC1KJkxowZumrVqr4WoyBkcq8lLJ82LzbhD+sILvU60Wrnc79YwSu++MRgZExlOds66akmQNUxpew52Jb1ff/3mo1c7lDD6EtEpFZVZ/TkGGEtn9uB54HXRKQR2AGMxg2W24CbpwPOArq3JwIZvUOm3m3gLJ+Eq2315r08+8b25D6KG3mdyJAbeYzVmRzJkGIuuIQA/+C2GROOS/ku/WRqYZSJzlymhtGfCaV8VHWDiJyOq7E5H9feph5YDjykqm3eup/0lqBGzwi61+Y92cCb21poj8WJiDDviqncevEpvLR2RzK7DdyF8qnVW2g53MbvXt/SdyfQh0QFVF1B295D6dbM+8YPZ+5lztj3D3F76W3nNhOBSESIe0P4InQv89AwBhKh3G4DmcHidktaPm1xMpXpRAU+fV41NeOG88DSd9m4+1DK+4O5eWhE4KwTh/tGV3dQVhLJWLgLqW4zIC2uY1aN0V/Jh9vNlM8AUz654gS1m5qZ92QDq3Nkq5UEqvANx0fPPIEX39pBXDVjVqBhDCZ6NebjdSz4pKquFpEN5L7xVVU9pSeCGD2ns5k80ydUMfXE4TmVT3tcQymeoSURDg+SVgcRca1vYnElGhHmzqnh+vM7Ok5ZYoBhdJ1cMZ+XcanTib8Ht4nUD/DHdfwB7fkrGnm6fis1Y4flbfrlQFU8AgyvKE2J7YyuLGf7/qMooJpac9OZwjcMIzO5Rirc6Pv7hoJIY/SIRHdqf23I/BWNfPO3awAGfZp0ZyTiWvs8xZN4nVA8mRIFsil8wzByEzbV2ugHZOpOfc/zb/e1WEVFNCJcd+5JDCsvoWHrfmrGDqNyaClVFWU8Xb+V//a1ujlhWIfFExF4/+Tj+fKHT0tRLpkUfi7MRWcYjtDKR0T+DLgL10B0BHCeqtaJyD8BS1X1md4R0egKwdqQ2VPHplk8UW+QTCzXeNIBiABfuPBk9h9t55f/s5H2WJyVG/ckXWVTxlSyYsMeWj2X4q4DRyktiSRrd4KKB3KPowhiLjrD6CDURDARuRA3ruB0YH5gvziu67XRDWo3NXPvknVZB7319LhTxlTykTNPSG4XXEr1b269gBEVpXn9zGJGgFsvmsRDyzayYEVjmqsMnCK5Zvr4jhk76qa6fvWjU3IqiukTqkJNGM3kojOMwUpYy+f7wB+BK4Eo8De+9+qAz+dXrMFBT++Es7lwgse9tGYMEa9Qsrw0wlVea/5TRx/Lyo35VXrFymfOr6ZyaCmt3mwd8DoTBFxlV08bz6K6jkaqV2fpBN4duuqiM4yBTFjlMw24yhvyFvTV7CJ701EjBz0JVmfrRp2IXSSOe7QtntKZ4NKaMQDJfSOQseh0oDF13HCmjKlMXvyjEdc4NThmoitutK7Sm8c2jP5GWOVzBKjI8t5YwHrsd4NMd8JhA9LBdjlzF9cTV02pwJfEf3zbnli9hYryEo62xQdV7nzzodbQF//e7Klm/doMwxFW+bwKfFlEFvu2Ja5dfwW8mFepBgmJi+GiuiYUWLuthXlPNYRyw/kVl3jJA0FlorjkAn+nalXY1XJ0QCgeAU4cMYSmvUdyrosKVFW4pqi5xoWbRWIYhSOs8rkL+G9gNfA47rr2lyLyr8B04NzeEW9wsLCuybnARJLWSzY3nP8imbiLzzVnRxXOGFPJW9taUFzMZyAoHgARGFIaTRp3XhJfqvXnvZ67uB4gpTNBAstCM4zCEyrbTVVX41KstwPfwv1/nkg6uFhV1/aOeAMfv/ssHlciIkQlc9fjxEXyR39cy3X3L2PtthZuv2QylUNLs467FuDdXQcB17dt7pyaATMaO66wbufBZAFoeWmEWz4wiYh3gokkC8W1DZq7uD5jVqFloRlG4cnV2+1MVX0j8VpV64APicgQ4Dhgr6oeyra/EY5g3CdXw8rl63cnYzWJiynAlr2HiUbA3/EmOI5Zgbgqv3x1Pe/uPFiw8ysU7z/VFYD6FUewjCke14zWZL6z0MyFZxidk8vtVi8iu4BXgKXe43VVPQIMzsEuvUBXMqBmThpJNCK0e1fV9rhy1+J6gp3J/dX44MYxJ+7s1/VTxdPZSIfZU8cmv7uykkjGhIqy0syKJZ9ZaObCM4xw5HK7/S0ukWAm8BNgFbBHRJ4Ska+LyPkiEi2EkAOdsEWK0ydUMe+KqUQjHY6zmDegzH+XXxKNMHvqWBbWNXH/y+/ygVNHcdaJwzMeU/qBD278iCF875NnURLtENYvdkRINvucPqGKuXNqiPi+o5IIfPb86rwUinaGufAMIxy5GoveizcSW0QmAxfj4j4fAD6OuxE9JCLLgZdV9bu9L+7gxe/Kue7ck1iwojGrJXD2+OHc/UR9ykTSkqikZb6Bd/cR6dtWO1HvFijDhGqiEeGvLzmV68+vZsqYShbWNSFAzbjhzHuqIaOrrPlQa9IaFOC6c6v53ifP6v0TwQpJDSMsYcdorwPWAb8AEJETccroU8BlwAcBUz69xPwVjck6npJohHPGD8/pgmo+1JaieADaY5rRyom5OQF5lbfLKHzwjBN4/o3tyfM6vrKM3QdaiceVeU81MGVMZVqa9JQxlRldZUEFkOjoUAiskNQwwtGlrtYiUo2zfhKP04ADuL5vRi9Qu6mZuYvrk3Ge1vY4r+VoiRMR2LjrQMb3eqpjSiJw84WTePDVDcm6okR68zFlUVqOxrLuK5L98+PqjlNeGkmO+d7V0jEzp7Utc9p5tpqdvlYAVkhqGJ2TU/mIyGmkKptqYAeu6PQ/vef/VdXB0KGlIAQzpZav353TJVYWFT5+1liWr9/NkNIoR9pibNufn4FxfiIC8644i+vPr6Z65DHJ4XTrdx3k2Te251Q8kFvxKfDS2zu5+7Ianq7fyqvv7Eqx7CIR6bL7yhSAYRQ3uVKttwKjgXdxBabzgFdU9Z0CyTboyJQpNXPSyKRFIIKXMu3WC3DOSSN48k9bez9moy6WUrupOdmFYeXGPUw5oTLU7on4f0LMspIIs04bxXOeqy0Wi9N8qJUvf/g0VqzfnXQbRiPCvCum9liRWPqzYRQXuSyfE4BDwJtAg/fYUAihBiuZMqVuv2Ryigtp7baWjvhPRFi1qTmtniVBZ+nJKWu9xdnWJ6yPoIwnDBtCZ639IgLfvfKsZOPTRB0TuDRwf3B++oQqFtxyQTKxINj4sztY+rNhFB+5lM8YOtxtf4Ebq3BERFbgan9eAZYVqtBURJ4BPgZ8T1W/7dteBfwQN+5hKC7+9BVVXVMIufJJtkypoAvpunNPSsZbHl7RmPFY502sYnhFGc+9sT3UZ/vdYseWRzl4NJaiiOa8z9XRrN3WQsTTVKUlEW69+BRmTRnNoysbqd+yLy1jTYAPn3FCMmEgSKbYTL5dZjbq2jCKj1yp1jtwfdweBxCR4XSkWl8KfNPb/r+4SaZf6y0hReQzwNkZtgvwBHAyri6pGbgTWCIi56hqU2/J1BsEG436qd3UzKK6Jh5btZn2uFJWEuGGCyYSkfRK/pKo8I3ZZ7B8/e6UDLKwHMgQv3ly9RYOtcZ4ae0OYnEl6rXqSSiK68+vpnZTMwvrmni8ton2mGt4KgLPv7mdpe/sTFocQRdYbysCS382jOIjdLabqu4DnvQeiMhM4A5cqvUMoFeUj4iMwBW5fgU3RdXP5cCFwAdVdYm3fhnOPfh14Eu9IVNvsnZbC4+u3Ewsriyqa+Lhm2cCbv7OkbYOs6K1Lc7PX92QNkLhwlOPZ/ZUl4BQVVFGeWnmav+uElN41mdFqWqysDNBQpFcPW08y9fv5r29h3nktca0gstCu8D6OvvNMIx0QikfEYngBsol3HAXAlW4690OXOud3uJfgAZVXSAimZTPloTiAackReRJ4AqKRPkk7vT98Y5sbf1T0qrbOi7Yre2p/iwV16ssQSJVefbUsSljGW64YGJKarSfaMRZTV1Nwc40AdRPQgklrDW/xdFXLjDLfjOM4iJXttuFdCibC4BjcdedJuAZnMJ5uTc7WnsyfJ4MLjePGqA+w/YG4PMicqyqZi56KRD+YLe/niXTHX8wrdqfYpwYt5AgqDDeN344cy+rSR0y1x6nYet+4uoUT0RgdGV5Ryq2wkfOOIEX39pBPK6UlbqR24tXb0kevyQCHzz9BF56eyexWJxoNMI108fnHC+daeyDX+GaC8wwjFyWT8KaWY+L+yzFxXYKkvEmIqXA/cCPcii444CNGbbv8Z6rcEWwwWPfAtwCUF2dPt8ln/iVAbhssmx3/FUVZSnWyc0XnpxcM++KqXz7d2tS0qwT8Z7SqDD3sppkQkBiTVyhZuwwVmzYQ1t7nJJohC996LSUtjS3XnwKt158SoqC+NwFE5Nxp4SSCVpv2cg03tuPucAMw4Dcyud6nGWztVDCBPgGLnvteznWZMsmztkuU1UfAB4AmDFjRq8WyCSC3QkFFIGsI7Prt+xLnlBEoHJoafI4U8ZU8iHPSlHPSsk0fqH5UGvHMYD9R9s7zCRVpoypTLv4B2fcZHJRJV53Fq8JWl6JtHD/enOBGYaRK9vtkUIK4sdr4/Mt4GagXETKfW+Xe0kILTgL57gMh0hc2bL3oSkQ/jv9YI1L0EJ4vLYjy63E53LzWxMlEeGciVUcbY/TuPtgioICkkWpCctGcKMXFNcBO1E7lLj4d6UGJky8Jtt4b0txNgzDT5d6uxWQScAQ4NcZ3vsH7/FnuNjORzOsORNo7Ot4T4JMd/r3LlmXciF/un4r7V6RjADXzjgpuU/KRT+myd5uq5v2pcWQgm4t8MZ0tzllUFVRliJHVxIAwqQsB5Vtts7ThmEMbopV+bwOXJJh+xKcQvoFrsv2E8CNInKxqr4MICLDcOnfwcy4oiJ4IZ89dSwrN+7J2InZvzbYhDqTVRFUdnPn1CTdX/4O0ZnkyKUgwsZr/J/vH4NgGIaRoCiVj6ruBV4Kbnc1pWxS1Ze810/gOhr8WkS+RkeRqeBStIuWTBfybCMC/GtbDrdx39L1yfc6S3sGFweKq2a0brqaANCdeM2iuiZa2+Ms9OqWzPVmGEZRKp+wqGpcROYAPwJ+hnPVLQMuUdXNfSpcCIIXcn99zL1L1mVtOePvKl05tDTU+O1c1k1vJgBYaxvDMDLRr5SPqqZ5b1R1D3CT9+j3hEkAmDKmMmexapC+TG+21jaGYWSiXymf/kJP2vcvX7872Q4n0xC17nZo7qv0ZqvrMQwjE6Z88kxP2/f7C03j3ms/+XJjFXK+jdX1GIYRxJRPnumpcmg+1JrsXBAR0roJ5MONZfNtDMPoa0z55JmeKocwyQE9dWNZEoBhGH2NKZ8801PlEGb/nrqxLAnAMIy+RrSr/fQHGDNmzNBVq1b1tRh5I2wsp5AxH8MwBhYiUquqM3pyDLN88kQxXMy7EsuxJADDMPoSUz55oFgC+BbLMQyjvxDpawEGApku+n1BIpYTlc5b7hiGYfQlZvnkge4G8PPtqrOCTsMw+guWcJCnhIOuKpJicdUZhmF0FUs4KCK6GsC3+IxhGIMZi/n0ERafMQxjMGOWTzfpabzG4jOGYQxmTPl0g3zFa6zWxjCMwYq53bpBsaRWG4Zh9FdM+XQDi9cYhmH0DHO7dQOL1xiGYfQMUz7dxOI1hmEY3cfcboZhGEbBMeVjGIZhFBxTPoZhGEbBMeVjGIZhFBxTPoZhGEbBMeVjGIZhFJxBP1JBRHYCm/paDh/HA7v6WogeMhDOAQbGedg5FAcD7RwmqOqonhxs0CufYkNEVvV0TkZfMxDOAQbGedg5FAd2DumY280wDMMoOKZ8DMMwjIJjyqf4eKCvBcgDA+EcYGCch51DcWDnEMBiPoZhGEbBMcvHMAzDKDimfAzDMIyCY8qnAIjIeBH5DxFZJiKHRERFZGI3jnOnt++rvSBmZ5/d7XPw1mZ6nNO7UmeUpUe/hYicISKPicguETksImtF5O96UeRMMnTrHETk7hy/xZECiO6XpSf/nqpF5Fci0ujt+7aIfFdEjullsYNy9OQcThaRx0Vkr4gcFJElIlLwVGwRuUZEForIJt+/538WkcoQ+w4RkR+KyFZv32UiclHYzzblUxgmA58CmoFXunMAEZkEfAvYkUe5ukJPz+Eh4ILA4+18CdcFun0e3sVhBVAO3Ax8HPgxEM2zjJ3R3XN4kPTf4MNAO/BEnmXsjG6dg6dgngcuAu4CPoE7r78Hfpl/MXPS3XMYCbwKTAVuBT7tvbVERM7It5Cd8A9ADPgmcCnwn8AXgedEpDP98AvgC8BcYA6wFfhj6JtKVbVHLz+AiO/vmwEFJnbxGH8E7gdeAl7tT+fgrf1uX/8OPTkP3I1aA/Db/noOWY71OW//T/SHcwA+6q39aGD793FKtKIfnMO3PVkn+7YdA2wHflPg32FUhm2f987lgzn2O9tbc6NvWwmwFngizGeb5VMAVDXek/1F5HpgGnBnfiTqOj09h2KhB+cxCzgT+Nf8SdM98vxb/CXuovfHPB6zU3pwDmXe8/7A9r24GwTprkxdpQfnMBN4R1XX+Y51EGc9zRGRgk2YVtWdGTav9J5PzLHr5UAb8KjvWO3AI8DHRKS8s8825VPkiEgV8BPg66q6p6/l6QFfFJGjnm/8RRH5QF8L1EUu9J6HiMhyEWkTkR0i8u8iMrRPJesmIjIeuAR42Ltw9AeeB94BfiAiZ4rIsSLyQeDvgPu8i3ixEwNaM2w/CgwFTimsOGlc7D2/mWNNDbBBVQ8FtjfgbhAmd/YhpnyKnx/iYiMP9bEcPeHXwF/j4gu3ACOBF0VkVh/K1FXGec+PAs8CHwH+Bedumd9XQvWQz+GuAb/qa0HCoqpHcDcCCTdoC/AC8BTwN30oWldYC5zqxX4A8OIr53kvj+sTqZwcJwLzgOdVdVWOpcfhYl1B9vjez0nBzDuj63jWweeBaeo5Vfsjqvo538tXRGQxUA98lw6LothJ3Kj9WlXnen+/JCJR4PsicqaqvtFHsnWXzwP/q6p/6mtBwiIiQ3A3AKNxyrMRd9Gei4ujfLHvpAvNfcCXgP8rIl8CDuGSiU723u8TF7eIHAssxn2PN3a2HBfzybQ9FGb5FDf34zJKmkRkhIiMwN0wRL3XnfpVixFVbQF+D5zb17J0gd3e83OB7c96z+cUTpSeIyLnAafTj6wej7/Cxd8+rqq/VtWlqvojXLbbbSJydp9KFwJVXQ98FpgOrAO24DIPf+It2VpomTyl/gQwCfiYqjZ1ssseMls3Vb73c2LKp7g5A7gNZ94mHu/HBSyb6R93ednIdudUrDR4z0GZE3d6/S0h4y9xd7j9zWV4FtCsqu8Gtr/mPRc6VblbqOpCXED/TFzW23TgWGCzqjYWUhYRKQUW4izIj6vqmhC7NQAni0hFYPuZuHjWuvRdUjHlU9xckuGxGueyugR4vO9E6z4iMgxXn7Gir2XpAk/jAsKXBrZ/zHvO5R8vKkSkDFdb8ocs2U7FzDagSkSCAe3zvef3CixPt1HVmKq+qarvisg44DpcnU3B8GJNDwMfAq5Q1eUhd30CKAWu9R2rBHcOz6rq0c4OYDGfAiEi13h/TveeZ4uborpTVV8WkQnAu8A8VZ0HoKovZTjOXqAk03u9TXfOQUT+AZgCLMG5FybgCtvG4FwPBaebv8VuEfln4C4R2Q+8CMzAxRp+5U+bLdZz8DEH5zLpU5dbN8/hIeCrwB9E5Hu4mM8MXMFpLfDfhZIfuv3/RCkuWeVlXMp4Da6MogFXtFxI7sUpkO8BB0Vkpu+9JlVtyvL/w+si8ihwj3c+G3CemJMJ+/91IQuaBvMD567J9HjJe3+i9/ruTo7zEn1QZNrdcwAuw10QduHqAnbj7prO62+/Bc7F9lWcS6EVN359HlDaX87Be2+x9zuU9dVv0MPf4UzgN8Bm4DAuG/RHQFV/OAfcTf9TuPqqo7gL+3cpYIGsT5aNOc7h7k5+h6G4urdtwBGcJ2NW2M+2kQqGYRhGwbGYj2EYhlFwTPkYhmEYBceUj2EYhlFwTPkYhmEYBceUj2EYhlFwTPkYhmEYBceUj1F0iMgN3kjitLbsIlLivXd3N457t4h0q7ZARF6SEOPLReRKEflqyGNO9M7lhu7I1FeIyJdF5Kq+lsPo35jyMQYTiTHSvcmVuELUMGzFyfP7XpOmd/gyYMrH6BHWXscYNKjr1NtZt96Coa7/VdheWv0SESnXEH2+jMGHWT7GgEBEThaRh0Vkpzcx9XUR+WRgTZrbTURGicgCEdkvIs0i8l8icrnnDpuV4XM+LCJ13kTWehG50vfeQ7hu0Sd6+6uIbMwhc5rbTUQeEpEmEfkzEXnF+5x3ROS2EN/BLO94V4rI/SKyxzunn4hIVETOFZFXReSgiDSIyMcyHONiEXlBRFq8dX8Ukam+9zfi+vN91neOD/m/XxGZ6u13ANcGBxGpEJEfiMgGEWn1nr/lNbZMHPtYEfkPEWn0fsPtIvK8iJze2bkb/Q+zfIxiJirp8+yjwUUichKur9QO4CvATlx33YUicqWqPpHjMxbh2vTfievZdjXwH1nWngL8G/DPuF51fw88LiKnq2ss+h1gFG5O0eXePt256x+GG3VwD6533I3Af4rIWlVdEmL/e3DndR1wEfBt3P/rH8ZNxn3P27ZIRCao6i4AEfkEru/b74G/8I71DdwAwPep6mbgk8AfcN3V7/bWBDtjL8bNofoBEPd+wz/ierJ9B1iDGwtyF67B6d97+/0E9719EzcqeyRuhMiIEOds9Df6srGgPeyR6QHcQPZmhylND731v8BdAEcGjvMc8Lrv9d3un3zy9Ue9Y30qsN8T3vZZvm0v4RqjnurbNhqIAd/0bXsI1w04zHlO9D7nhsD+Clzi21aOU3YPdHK8Wd6+vwxsr/O2X+jb9j5v21/6tq0DXgjsO8z77Ht82zbiJroGP/9u75h/F9j+OW/7RYHt38I1aB3tva4H/rWv//3ZozAPc7sZxcwncVaE/zEzw7pLcXfj+7xsuBLf3fbZ4uYHZWImTnn8NrA925ykd1T1ncQLVd2Bs7aqQ55PWA6pz8JRFzN5pwuf83Tg9VvAQVV9NbAN4CQAETkVZ9k9HPgODwHLcBZUWILf56W4DuD/Ezj2s7iZMInfdCVwg4h8U0RmiBtRbgxQzO1mFDP1GpiTk8ENB84C+bz3yMRI3NyUIGNxUzHbAtu3ZzlOptHAR4EhWdZ3l+Yefk5w/1Zgr3+DqraKCL5jjvaef+E9gnRlumZwDPRoXJwo+D0nGOk9/y2uPf9NuPkye0Tk/wLfUtVDXfh8ox9gyscYCOwGXsHFGDKxJcv2rbipmKUBBXRCPoXrJ+z2nu8Ens/wfmsXjhWspdqNGzb2qSzrNwKo6gHv8+8UN8DsGuD73md/owufb/QDTPkYA4FncPUyDap6uAv7LcclMHwSLyvL49rMy0NxFDdkq7+xFqcEalT1+52s7eo5PoNL5Digqm91thhAVTcBPxaRzwJTO1tv9D9M+RgDgbnAa8BSEfkp7iJahbtoTVLVmzLtpKrPel0LHhCR43EB92uAs70l8W7I8gZwnIh8EVgFHFHVNd04TkFRVRWR24HFIlKGU8a7cFbgnwONqvqv3vI3gA+IyBycm2yXqm7McfiHcRl7L4jIj3GZcmW4GNPlwJWqekhEluGSPdYAB4CLcb9Fn477NnoHUz5Gv0dVG0VkBi7b6p9w6c67cdlTnV24rsKlVv8Al3zwBC4F+CFgXzfEeRAXQP8nXIrwJlxWW9Gjqn8QkYtwWWgP4qybbTgL8VHf0juBn+MU1FDcd3xDjuO2eTVFdwC3ACcDB3Hjo39Ph0tvKc41dwfu2rQe+Iqq/nt+ztAoJmyMtmEEEJF7cRfT49Sq8w2jVzDLxxjUeN0FhgMNOFfQpcBtwA9N8RhG72HKxxjsHMQ1yjwFV8y5AVdh/8M+lMkwBjzmdjMMwzAKjnU4MAzDMAqOKR/DMAyj4JjyMQzDMAqOKR/DMAyj4JjyMQzDMArO/wdzI+NdEzcxWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_loader import load_data\n",
    "height, weight, biological_sex = load_data()\n",
    "biological_sex = np.double(biological_sex).reshape(-1, 1)\n",
    "plt.plot(height, weight, '.', linewidth=3)\n",
    "plt.xlabel('Height in metres', fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.ylabel('Weight in kilogram', fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tight_layout;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93ddcbc533969ca2585a24d87b6f10ff",
     "grade": false,
     "grade_id": "cell-1d8cb3da4dac45fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following cell, write code that initialises a polynomial data matrix _data_matrix_ of degree one with the standardised inputs formed from the height and weight arrays of the dataset. Define an objective function _objective_ with argument _weights_ based on the **binary_logistic_regression_cost_function** with fixed arguments _data_matrix_ and _biological_sex_. Repeat the same exercise to create a function _gradient_ based on **binary_logistic_regression_gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7d63d2842004f750b77a6454bf5006d",
     "grade": false,
     "grade_id": "cell-65673f36b848eab4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.94406149  2.50579697]\n",
      " [ 1.          0.62753668  0.02710064]\n",
      " [ 1.          2.01244346  1.59780623]\n",
      " ...\n",
      " [ 1.         -0.64968792 -1.02672965]\n",
      " [ 1.          0.69312469  0.07512745]\n",
      " [ 1.         -1.14970831 -1.48850724]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#creating the polynomial basis array\n",
    "height_standardised = standardise(height)\n",
    "weight_standardised = standardise(weight)\n",
    "weight_height_std = np.c_[height_standardised,weight_standardised]\n",
    "data_matrix = polynomial_basis(weight_height_std)\n",
    "#checking the polynomial array\n",
    "print(data_matrix)\n",
    "\n",
    "#creating the cost function and gradient\n",
    "objective = lambda weights: binary_logistic_regression_cost_function(data_matrix, weights, biological_sex)\n",
    "gradient = lambda weights: binary_logistic_regression_gradient(data_matrix, weights, biological_sex)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "726095861ab7131322e489c8a33d15f5",
     "grade": false,
     "grade_id": "cell-5c25ca5d2e19856b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Call gradient descent with the following cell to compute _optimal_weights_ for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7753f6ed0ac1f5a4adbb1ef268caae81",
     "grade": false,
     "grade_id": "cell-fb31b6af5bbbcc9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000/7000, objective = 2091.9568591203943.\n",
      "Iteration 2000/7000, objective = 2091.30072155257.\n",
      "Iteration 3000/7000, objective = 2091.297983425523.\n",
      "Iteration 4000/7000, objective = 2091.2979712556394.\n",
      "Iteration 5000/7000, objective = 2091.2979712013266.\n",
      "Iteration 6000/7000, objective = 2091.297971201084.\n",
      "Iteration 7000/7000, objective = 2091.297971201083.\n",
      "Iteration completed after 7000/7000, objective = 2091.297971201083.\n"
     ]
    }
   ],
   "source": [
    "initial_weights = np.zeros((data_matrix.shape[1], 1))\n",
    "optimal_weights, objective_values = gradient_descent(objective, gradient, initial_weights, \\\n",
    "                                    step_size=1.9/(np.linalg.norm(data_matrix, 2) ** 2), \\\n",
    "                                    no_of_iterations=7000, print_output=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c84469bd2889feaf8a713f0dfb3d716a",
     "grade": false,
     "grade_id": "cell-da55ede3853660cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A correct result of your gradient-descent-based logistic regression strategy will be awarded **4 marks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0102eed55ffb0f81e5ba3f42cffd9422",
     "grade": true,
     "grade_id": "cell-8090fbc611c0a194",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal weights are w = [[-0.01870311  1.89527459 -6.3680829 ]].T with objective value L(w) = 2091.297971201083.\n"
     ]
    }
   ],
   "source": [
    "print(\"The optimal weights are w = {w}.T with objective value L(w) = {o}.\".format(w = optimal_weights.T, \\\n",
    "        o=objective_values[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14f46ce484ad38b6e849796544e9ad6c",
     "grade": false,
     "grade_id": "cell-18f1b30c908aaae8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write two functions **prediction_function** and **classification_accuracy** that turn your predicitons into classification results and that compare how many labels have been classified correctly. The function **prediction_function** takes the arguments _data_matrix_ and _weights_ as inputs and returns a vector of class labels with binary values in $\\{0, 1\\}$ as its output. The function **classification_accuracy** takes two inputs _true_labels_ and _recovered_labels_ and returns the percentage of correctly classified labels divided by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dda3261c6408de6bfac72ee8a2e3d197",
     "grade": false,
     "grade_id": "prediction-and-accuracy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def prediction_function(data_matrix, weights):\n",
    "    results = logistic_function(data_matrix @ weights)\n",
    "    binary = np.where(results >0.5,1,0)\n",
    "    return binary\n",
    "   \n",
    "def classification_accuracy(true_labels, recovered_labels):\n",
    "    equal_labels = (recovered_labels == true_labels)\n",
    "    return np.mean(equal_labels)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd533efa2d5716dd04fe9c649cb9f0bf",
     "grade": false,
     "grade_id": "cell-747f695d7dde1390",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The correct classification accuracy is awarded **4 marks**. The total marks possible in this section are **13 marks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "977c57bb1188016adfa151474f8baecf",
     "grade": true,
     "grade_id": "accuracy-test",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the training set is 91.94 %.\n"
     ]
    }
   ],
   "source": [
    "print(\"The classification accuracy for the training set is {p} %.\".format(p = 100 * \\\n",
    "        classification_accuracy(biological_sex, prediction_function(data_matrix, optimal_weights))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00633505cdd1f8712777a761fbe95961",
     "grade": false,
     "grade_id": "cell-66ec567aec881880",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Multinomial logistic regression\n",
    "\n",
    "This concludes the binary classification part of the first part of the final assessment. We now move on to multinomial logistic regression for multi-class classfication problems. As a first exercise, implement the softmax function **softmax_function** as defined in the lectures. The function takes the NumPy array _argument_ as its main argument, but also has an optional _axis_ argument to determine across which array-dimension you apply the softmax operation. If this argument is not specified (or set to _None_), then the softmax operation is applied to the entire array. Make sure your function works at least for NumPy arrays _argument_ with arbitrary numerical values and dimension one or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4977cb40c6c166eebb31fe6ef824918a",
     "grade": false,
     "grade_id": "softmax",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76528029 0.71807976]\n",
      " [0.23049799 0.01775346]\n",
      " [0.00422172 0.26416678]]\n"
     ]
    }
   ],
   "source": [
    "def softmax_function(argument, axis=None):\n",
    "    \n",
    "    if argument.ndim == 1:\n",
    "        new_array = np.exp(argument)\n",
    "        result = new_array/np.sum(new_array)\n",
    "        \n",
    "        return result\n",
    "    elif argument.ndim > 1:\n",
    "        \n",
    "        if axis == None:\n",
    "            result = np.exp(argument)/sum(sum(np.exp(x)) for x in argument)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        elif axis == 0:\n",
    "            new_array = np.exp(argument)\n",
    "            result = new_array/new_array.sum(axis=0, keepdims = True)\n",
    "            \n",
    "            return result\n",
    "                \n",
    "        elif axis == 1:\n",
    "            new_array = np.exp(argument)\n",
    "            result = new_array/new_array.sum(axis=1, keepdims = True)\n",
    "          \n",
    "            return result\n",
    "    \n",
    "print(softmax_function(np.array([[1.5,3], [0.3,-0.7], [-3.7,2]]),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d93ee498f6cc2c5167fb18f05b55cb72",
     "grade": false,
     "grade_id": "cell-0bd9e5f6e5931b4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your softmax function with the following cell. Passing this test is awarded with **4 marks**. Please note that, as usual, some tests are hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92dadfc8cfb35300c37378165909b4a0",
     "grade": true,
     "grade_id": "cell-b7751e0278da3533",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The softmax of [[ 1.5  0.3 -3.7]].T is [[0.76528029 0.23049799 0.00422172]].T.\n"
     ]
    }
   ],
   "source": [
    "argument = np.array([[1.5], [0.3], [-3.7]])\n",
    "print(\"The softmax of {arg}.T is {out}.T.\".format(arg=argument.T, out=softmax_function(argument).T))\n",
    "assert_array_almost_equal(softmax_function(np.array([[1.5], [0.3], [-3.7]])), np.array([[0.76528029], \\\n",
    "                                                        [0.23049799], [0.00422172]]))\n",
    "assert_array_almost_equal(softmax_function(np.array([[1.5, 3], [0.3, -0.7], [-3.7, 2]]), axis=0), \\\n",
    "                          np.array([[0.76528029, 0.71807976], [0.23049799, 0.01775346], \\\n",
    "                                    [0.00422172, 0.26416678]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22c6fcff95effedaa57877d34de89392",
     "grade": false,
     "grade_id": "cell-941464e24613e0a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, write a function **one_hot_vector_encoding** that converts an NumPy array _labels_ with values in the range of $\\{0, K - 1\\}$ into so-called one-hot vector encodings. For example, for $K = 3$ and a label vector $\\text{labels} = \\left( \\begin{matrix} 2 & 0 & 1 & 2\\end{matrix} \\right)^\\top$, the output of **one_hot_vector_encoding(labels)** should be a two-dimensional NumPy array of the form\n",
    "\n",
    "\\begin{align*}\n",
    "\\left( \\begin{matrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{matrix} \\right) \\, . \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94bcefd8f6f2834778c5e510f9797c83",
     "grade": false,
     "grade_id": "one-hot-vector-encoding",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_vector_encoding(labels):\n",
    "    a = labels\n",
    "    b = np.zeros((a.size, a.max()+1))\n",
    "    b[np.arange(a.size),a] = 1\n",
    "    return b   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "740154e524df161eed6ffcb3b6daf01f",
     "grade": false,
     "grade_id": "cell-4d72a85842f6c692",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement the cost function and gradient for the multinomial logistic regression in terms of two functions **multinomial_logistic_regression_cost_function** and **multinomial_logistic_regression_gradient**. As in the binary classification case, the arguments are the polynomial data matrix _data_matrix_ and weights that are now named _weight_matrix_. Instead of passing on labels as _outputs_ as in the binary case, you pass the one hot vector encoding representation _one_hot_vector_encodings_ as your third argument. Return the cost function value, respectively the gradient, following the mathematical formulas in the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6866c7079139fd8a29d4c148a6e2f034",
     "grade": false,
     "grade_id": "multinomial_logistic_regression",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multinomial_logistic_regression_cost_function(data_matrix, weight_matrix, one_hot_vector_encodings):\n",
    "    #loss function\n",
    "    s = data_matrix.shape[0]\n",
    "    K = one_hot_vector_encodings.shape[1]\n",
    "    values = 0\n",
    "    \n",
    "    for i in range(s):\n",
    "        term1s = []\n",
    "        col = np.where(one_hot_vector_encodings[i,:] == 1)[0][0]\n",
    "        term2 = (np.dot(data_matrix[i,:],weight_matrix[:,col]))\n",
    "        for j in range(K): \n",
    "            term1_before = (np.exp(np.dot(data_matrix[i,:],weight_matrix[:,j])))                  \n",
    "            term1s.append(term1_before)\n",
    "        term1 = np.log((np.sum(term1s)))\n",
    "        values += term1 - term2\n",
    "    return values\n",
    "    \n",
    "def multinomial_logistic_regression_gradient(data_matrix, weight_matrix, one_hot_vector_encodings):\n",
    "    result = data_matrix.T @ (softmax_function(data_matrix@weight_matrix,axis = 1) - one_hot_vector_encodings)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a034361a28a9e882f7d7ad4328070a9c",
     "grade": false,
     "grade_id": "cell-b810132e15131dc9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your implementation on the [UCI wine dataset](https://archive.ics.uci.edu/ml/datasets/wine); the dataset contains 13 attributes from a chemical analysis of Italian wines from three different cultivars. For more information on the dataset visit [this link](https://archive.ics.uci.edu/ml/datasets/wine). The code in the following cell loads the dataset and stores the labels in a NumPy array _labels_ and the attributes in a NumPy array _inputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c66a8b68f9903af0351c13f30605934f",
     "grade": false,
     "grade_id": "cell-d080a23a2048227b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "wines = np.loadtxt('wine.data', delimiter=',')\n",
    "labels = wines[:, 0].astype(int) - 1\n",
    "inputs = wines[:, 1::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a9db194ed9b4c4b458c753d58b0196e",
     "grade": false,
     "grade_id": "cell-d26a44b054d22e4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Transform the labels _labels_ into a one hot vector representation with your function **one_hot_vector_encoding** and store your results in a NumPy array named _outputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9378cf561d0799b518319252eaeb6c87",
     "grade": false,
     "grade_id": "cell-7362e79b5354d63b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "outputs = one_hot_vector_encoding(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a365aa01296ecd58f3550a5a83dc403d",
     "grade": false,
     "grade_id": "cell-100a0ea7469b3821",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following cell, write code that initialises a polynomial data matrix _data_matrix_ of degree one with the standardised inputs _inputs_ from the wine dataset. Define an objective function _objective_ with argument _weight_matrix_ based on the **multinomial_logistic_regression_cost_function** with fixed arguments _data_matrix_ and _one_hot_vector_encodings_. Repeat the same exercise to create a function _gradient_ based on **multinomial_logistic_regression_gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "712836b79b8e6389ce84a45f8439a300",
     "grade": false,
     "grade_id": "cell-094a5bddd94a009a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "data_matrix = polynomial_basis(standardise(inputs))\n",
    "\n",
    "objective = lambda weight_matrix: multinomial_logistic_regression_cost_function(data_matrix, weight_matrix, outputs) \n",
    "gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(data_matrix, weight_matrix, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d06a10bf709742803be0723fd183611d",
     "grade": false,
     "grade_id": "cell-901f918df8fdef00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Call gradient descent with the following cell to compute an _optimal_weight_matrix_ for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acf5d4074e6b0b65252ef485d45d5838",
     "grade": false,
     "grade_id": "cell-175babea73e2db36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000/100000, objective = 0.37363264592740286.\n",
      "Iteration 10000/100000, objective = 0.19859836381135887.\n",
      "Iteration 15000/100000, objective = 0.13605627122241737.\n",
      "Iteration 20000/100000, objective = 0.10372479744703211.\n",
      "Iteration 25000/100000, objective = 0.08391801536513466.\n",
      "Iteration 30000/100000, objective = 0.07051872085948618.\n",
      "Iteration 35000/100000, objective = 0.06084138658523397.\n",
      "Iteration 40000/100000, objective = 0.05351961411115225.\n",
      "Iteration 45000/100000, objective = 0.047784005082136094.\n",
      "Iteration 50000/100000, objective = 0.04316789618475969.\n",
      "Iteration 55000/100000, objective = 0.03937164510108371.\n",
      "Iteration 60000/100000, objective = 0.036193967173282715.\n",
      "Iteration 65000/100000, objective = 0.0334945959405184.\n",
      "Iteration 70000/100000, objective = 0.031172765703073146.\n",
      "Iteration 75000/100000, objective = 0.02915420629986798.\n",
      "Iteration 80000/100000, objective = 0.027382962414542966.\n",
      "Iteration 85000/100000, objective = 0.02581606984776652.\n",
      "Iteration 90000/100000, objective = 0.024419988124662062.\n",
      "Iteration 95000/100000, objective = 0.02316814830816938.\n",
      "Iteration 100000/100000, objective = 0.022039229212158062.\n",
      "Iteration completed after 100000/100000, objective = 0.022039229212158062.\n"
     ]
    }
   ],
   "source": [
    "initial_weight_matrix = np.zeros((data_matrix.shape[1], outputs.shape[1]))\n",
    "optimal_weight_matrix, objective_values = gradient_descent(objective, gradient, initial_weight_matrix, \\\n",
    "                                    step_size=1.9/(np.linalg.norm(data_matrix, 2) ** 2), \\\n",
    "                                    no_of_iterations=100000, print_output=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5a8d433e17eb215eab8b141e3fa48bd",
     "grade": false,
     "grade_id": "cell-b7bf0daf68390bac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a function **multinomial_prediction_function** that turns your predicitons into labels. The function takes the arguments _data_matrix_ and _weight_matrix_ as inputs and returns a vector of labels with values in $\\{0, K - 1 \\}$ as its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e824634474ea1ee537d80abc1bd092cd",
     "grade": false,
     "grade_id": "cell-2da6bb5a7e17ea5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multinomial_prediction_function(data_matrix, weight_matrix):\n",
    "    k = weight_matrix.shape[1]\n",
    "    matrix = logistic_function(data_matrix@weight_matrix)\n",
    "    ohv_matrix = (matrix == matrix.max(axis=1,keepdims = 1)).astype(float)\n",
    "    prediction = ohv_matrix @ np.array(np.arange(0,k))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4550fa0d0fcc9f5244cf0ca66c5bf8a5",
     "grade": false,
     "grade_id": "cell-9036bda8b3b0ced4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The correct classification accuracy is awarded **4 marks**. The total number of possible marks in this section is **8 marks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbf3d6b935f497fab4bd93eeea119319",
     "grade": true,
     "grade_id": "cell-49fe4e66273baf80",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the wine training set is 100.0 %.\n"
     ]
    }
   ],
   "source": [
    "print(\"The classification accuracy for the wine training set is {p} %.\".format(p = 100 * \\\n",
    "        classification_accuracy(labels, multinomial_prediction_function(data_matrix, optimal_weight_matrix))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba0705811c857f5094055fbd9f385917",
     "grade": false,
     "grade_id": "cell-45dcd721e2869a9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ridge logistic regression\n",
    "\n",
    "For the next part, modify the multinomial logistic regression problem to include a squared Frobenius norm of the weights as a regularisation term, similar to ridge regression where we added a multiple of the squared Euclidean norm of the weights to the mean squared error. Write two functions **ridge_logistic_regression_cost_function** and **ridge_logistic_regression_gradient** that take the arguments _data_matrix_, _weight_matrix_, _one_hot_vector_encodings_ and _regularisation_parameter_ as inputs. The function **ridge_logistic_regression_cost_function** returns the evulation of the multinomial logistic regression cost function with its linear model being determined by the polynomial basis matrix _data_matrix_ and the weight matrix _weight_matrix_, plus _regularisation_parameter_ times the squared Frobenius norm of _weight_matrix_ divided by two. The function **ridge_logistic_regression_gradient** is supposed to compute the corresponding gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acdbb33c938f33ee566b9dee3262d846",
     "grade": false,
     "grade_id": "cell-abadca310df6699f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ridge_logistic_regression_cost_function(data_matrix, weight_matrix, one_hot_vector_encodings, \\\n",
    "                                            regularisation_parameter):\n",
    "    ridge_term = (regularisation_parameter/2) * (np.linalg.norm(weight_matrix)**2)\n",
    "    return multinomial_logistic_regression_cost_function(data_matrix, weight_matrix, one_hot_vector_encodings) + ridge_term\n",
    "    \n",
    "    \n",
    "    \n",
    "def ridge_logistic_regression_gradient(data_matrix, weight_matrix, one_hot_vector_encodings, \\\n",
    "                                       regularisation_parameter):\n",
    "    ridge_term = regularisation_parameter * weight_matrix\n",
    "    return multinomial_logistic_regression_gradient(data_matrix,weight_matrix,one_hot_vector_encodings) + ridge_term \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0996e4e5192b31e3a4934a13a883b09",
     "grade": false,
     "grade_id": "cell-1af3c4a5f531e2fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Set your regularisation parameter _regularisation_parameter_ to the value 15 and define an objective function _objective_ as well as a gradient function _gradient_, both with argument _weight_matrix_, for fixed _data_matrix_ and _outputs_ as from the wine dataset that you have used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f40622d6042d8793ce03206d450ee648",
     "grade": false,
     "grade_id": "cell-3f1796626df02a88",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "regularisation_parameter = 15\n",
    "objective = lambda weight_matrix: ridge_logistic_regression_cost_function(data_matrix, weight_matrix, outputs,regularisation_parameter) \n",
    "gradient = lambda weight_matrix: ridge_logistic_regression_gradient(data_matrix, weight_matrix, outputs,regularisation_parameter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d13bd2ced951479730e1afad72283eb",
     "grade": false,
     "grade_id": "cell-a0c298f30e585bd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your solution with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d5eec1441b9057a16855a9953bfe215",
     "grade": false,
     "grade_id": "cell-4ac1dd08e2ba7c80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/100, objective = 50.66582189248285.\n",
      "Iteration 20/100, objective = 48.11155251656332.\n",
      "Iteration 30/100, objective = 47.8219334972513.\n",
      "Iteration 40/100, objective = 47.766837844563554.\n",
      "Iteration 50/100, objective = 47.752421479679114.\n",
      "Iteration 60/100, objective = 47.747879295725696.\n",
      "Iteration 70/100, objective = 47.7462923106324.\n",
      "Iteration 80/100, objective = 47.745701101573275.\n",
      "Iteration 90/100, objective = 47.74547049334518.\n",
      "Iteration 100/100, objective = 47.74537725711093.\n",
      "Iteration completed after 100/100, objective = 47.74537725711093.\n"
     ]
    }
   ],
   "source": [
    "initial_weight_matrix = np.zeros((data_matrix.shape[1], outputs.shape[1]))\n",
    "ridge_weight_matrix, ridge_objective_values = gradient_descent(objective, gradient, initial_weight_matrix, \\\n",
    "                                    step_size=1.9/np.linalg.norm(data_matrix.T @ data_matrix + \\\n",
    "                                    regularisation_parameter * np.eye(data_matrix.shape[1]), 2), \\\n",
    "                                    no_of_iterations=100, print_output=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2920f8564006d977e1d1f3cc19c46989",
     "grade": false,
     "grade_id": "cell-ad40701d626892d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The correct classification accuracy is awarded **4 marks**, which is also the total number of possible points in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a093e26a90163695684c1f98e8cfd645",
     "grade": true,
     "grade_id": "cell-3d84066ed6f97945",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ridge regression classification accuracy with regularisation parameter 15 for the wine training set is 99.43820224719101 %.\n"
     ]
    }
   ],
   "source": [
    "print(\"The ridge regression classification accuracy with regularisation parameter {a}\".format(a = \\\n",
    "       regularisation_parameter), \"for the wine training set is {p} %.\".format(p = 100 * \\\n",
    "        classification_accuracy(labels, multinomial_prediction_function(data_matrix, ridge_weight_matrix))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e59581e2eed10dab4a2d0ca63e5e67d",
     "grade": false,
     "grade_id": "cell-681ea7611e1b8d47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## LASSO-type logistic regression\n",
    "\n",
    "As a final exercise before you are left to continue with the main part of your project of classifying handwritten digits, implement a modification of the multinomial logistic regression problem that contains a positive multiple of the one-norm of the weight matrix as regularisation term, and approximate a solution numerically with the proximal gradient descent method as introduced in the lectures. You can make use of computational solutions for coursework 9.\n",
    "\n",
    "Begin by completing the following three functions. The function **soft_thresholding** takes the two arguments _argument_ and _threshold_ and returns the solution of the soft-thresholding operation applied to _argument_ with threshold _threshold_. \n",
    "\n",
    "The function **lasso_logistic_regression_cost_function** is supposed to implement the multinomial logistic regression loss with additional one-norm regularisation for polynomial basis matrix _data_matrix_, weight matrix _weight_matrix_, the one hot vector encoding output _one_hot_vector_encodings_ and the regularisation parameter _regularisation_parameter_.\n",
    "\n",
    "The function **proximal_gradient_descent** takes the same arguments as the function **gradient_descent**, with additional argument _proximal_map_ in order to specify the proximal map to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdab43cdab7db63c950b7d0a6c06e564",
     "grade": false,
     "grade_id": "cell-b2608195afc72125",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def soft_thresholding(argument, threshold):\n",
    "    return np.sign(argument) * np.maximum(0, np.abs(argument) - threshold)\n",
    "    \n",
    "def lasso_logistic_regression_cost_function(data_matrix, weight_matrix, one_hot_vector_encodings, \\\n",
    "                                       regularisation_parameter):\n",
    "    lasso_term = regularisation_parameter * np.linalg.norm(weight_matrix,1)\n",
    "    return multinomial_logistic_regression_cost_function(data_matrix, weight_matrix, one_hot_vector_encodings) + lasso_term\n",
    "    \n",
    "def proximal_gradient_descent(objective, gradient, proximal_map, initial_weights, step_size=1, \\\n",
    "                              no_of_iterations=100, print_output=100):\n",
    "    objective_values = []\n",
    "    weights = initial_weights    \n",
    "    objective_values.append(objective(weights))\n",
    "    for counter in range(no_of_iterations):\n",
    "        weights = proximal_map(weights - step_size * gradient(weights))\n",
    "        objective_values.append(objective(weights))\n",
    "        if (counter + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=counter+1, \\\n",
    "                    m=no_of_iterations, o=objective_values[counter]))\n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=counter + 1, \\\n",
    "                m=no_of_iterations, o=objective_values[counter]))\n",
    "    return weights, objective_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bfbe1872c304938510aeafc7fa10c25",
     "grade": false,
     "grade_id": "cell-e3ac148e879c476f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the next cell, define a suitable objective function _objective_ and a suitable gradient function _gradient_, both with argument _weight_matrix_ and fixed _data_matrix_ and _outputs_ from the wine dataset that you have used before, as well as a suitable proximal map function _proximal_map_ with correctly chosen threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5af740fe918364e9026fad9423073f3d",
     "grade": false,
     "grade_id": "cell-3afeb025acbc0916",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "regularisation_parameter = 0.5\n",
    "step_size = 1.9/(np.linalg.norm(data_matrix, 2) ** 2)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "objective = lambda weight_matrix: lasso_logistic_regression_cost_function(data_matrix, weight_matrix, outputs, \\\n",
    "                                       regularisation_parameter)\n",
    "gradient = lambda weight_matrix: multinomial_logistic_regression_gradient(data_matrix, weight_matrix, outputs)\n",
    "\n",
    "proximal_map = lambda weights: soft_thresholding(weights, regularisation_parameter * \\\n",
    "                                                 step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8da58316078974a165e9e3fd00369e62",
     "grade": false,
     "grade_id": "cell-59e703fdde6e42f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your solution with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "998771495b551e63b22c8b1980b179c9",
     "grade": false,
     "grade_id": "cell-33117d62e5699048",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000/10000, objective = 8.24959786201634.\n",
      "Iteration 2000/10000, objective = 8.263886291899661.\n",
      "Iteration 3000/10000, objective = 8.320757757764321.\n",
      "Iteration 4000/10000, objective = 8.354235362086085.\n",
      "Iteration 5000/10000, objective = 8.374118326110752.\n",
      "Iteration 6000/10000, objective = 8.385347232389174.\n",
      "Iteration 7000/10000, objective = 8.392015832216455.\n",
      "Iteration 8000/10000, objective = 8.396214878066097.\n",
      "Iteration 9000/10000, objective = 8.399020949261885.\n",
      "Iteration 10000/10000, objective = 8.40100378273745.\n",
      "Iteration completed after 10000/10000, objective = 8.40100378273745.\n"
     ]
    }
   ],
   "source": [
    "initial_weight_matrix = np.zeros((data_matrix.shape[1], outputs.shape[1]))\n",
    "lasso_weight_matrix, lasso_objective_values = proximal_gradient_descent(objective, gradient, proximal_map, \\\n",
    "                                                initial_weight_matrix, step_size, no_of_iterations=10000, \\\n",
    "                                                print_output=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e56343b5f34fafa2ed80f9b572b528f",
     "grade": false,
     "grade_id": "cell-69a32c9f26d37f47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The correct classification accuracy is awarded **5 marks**, which is also the total number of points available in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0f049b162367cd9d765fb16be5a693e",
     "grade": true,
     "grade_id": "cell-4cd3a07610421486",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lasso classification accuracy with regularisation parameter 0.5 for the wine training set is 100.0 %.\n"
     ]
    }
   ],
   "source": [
    "print(\"The lasso classification accuracy with regularisation parameter {a}\".format(a = \\\n",
    "       regularisation_parameter), \"for the wine training set is {p} %.\".format(p = 100 * \\\n",
    "        classification_accuracy(labels, multinomial_prediction_function(data_matrix, lasso_weight_matrix))))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
